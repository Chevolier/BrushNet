{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e78294-3a93-4f1b-8070-f8d2d138504b",
   "metadata": {},
   "source": [
    "# Text to Fashion Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a473fe8-ef6b-49a4-b7d2-d96600d9163d",
   "metadata": {},
   "source": [
    "## 1. Upload data to S3\n",
    "Here I use pokeman dataset as an example, which is composed of 833 image-text pairs. To scale up, you can just process your data into the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f972fb3-9c30-4c5c-adea-36ed78e38d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56b6cc0-322e-472d-8dd3-772a94305cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-420486383638/example_data\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "prefix = 'example_data'\n",
    "inputs_train = sagemaker_session.upload_data(path = \"example_data\", key_prefix=prefix)\n",
    "print(inputs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bb61d-6527-4707-9cbb-185c89b51008",
   "metadata": {},
   "source": [
    "## 2. Start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a06ebc-ab38-48b6-9e9e-9c3f0145440e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: t2i-acc-launch-2-2024-03-22-16-22-47-061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-22 16:22:50 Starting - Starting the training job...\n",
      "2024-03-22 16:22:57 Pending - Training job waiting for capacity...\n",
      "2024-03-22 16:23:23 Pending - Preparing the instances for training...\n",
      "2024-03-22 16:24:11 Downloading - Downloading input data......\n",
      "2024-03-22 16:25:11 Downloading - Downloading the training image............\n",
      "2024-03-22 16:27:22 Training - Training image download completed. Training in progress......\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-03-22 16:28:01,845 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-03-22 16:28:01,901 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-03-22 16:28:01,911 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-03-22 16:28:01,913 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-03-22 16:28:03,192 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting torch==2.0.1 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for torch==2.0.1 from https://files.pythonhosted.org/packages/e5/9a/ce0fe125f226ffce8deba6a18bd8d0b9f589aa236780a83a6d70b5525f56/torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate>=0.16.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.21.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.14.1+cu117)\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-22 16:28:01,347 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-22 16:28:01,404 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-22 16:28:01,414 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-22 16:28:01,416 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-22 16:28:02,914 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting torch==2.0.1 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torch==2.0.1 from https://files.pythonhosted.org/packages/e5/9a/ce0fe125f226ffce8deba6a18bd8d0b9f589aa236780a83a6d70b5525f56/torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.16.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.14.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting transformers>=4.25.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers>=4.25.1 from https://files.pythonhosted.org/packages/a4/73/f620d76193954e16db3d5c53a07d956d7b9c800e570758d3bff91906d4a4/transformers-4.39.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting ftfy (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for ftfy from https://files.pythonhosted.org/packages/f4/f0/21efef51304172736b823689aaf82f33dbc64f54e9b046b75f5212d5cee7/ftfy-6.2.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.7.0 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft==0.7.0 from https://files.pythonhosted.org/packages/82/cc/bf022d6bc3996a5939c3ee39bde2b0e1f8bf6cea6ef9c9cdaf1639586237/peft-0.7.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed>=0.9.3 (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 63.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting transformers>=4.25.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for transformers>=4.25.1 from https://files.pythonhosted.org/packages/a4/73/f620d76193954e16db3d5c53a07d956d7b9c800e570758d3bff91906d4a4/transformers-4.39.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 15.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting datasets (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[35mCollecting ftfy (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for ftfy from https://files.pythonhosted.org/packages/f4/f0/21efef51304172736b823689aaf82f33dbc64f54e9b046b75f5212d5cee7/ftfy-6.2.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (3.1.2)\u001b[0m\n",
      "\u001b[35mCollecting peft==0.7.0 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for peft==0.7.0 from https://files.pythonhosted.org/packages/82/cc/bf022d6bc3996a5939c3ee39bde2b0e1f8bf6cea6ef9c9cdaf1639586237/peft-0.7.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[35mCollecting deepspeed>=0.9.3 (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mDownloading deepspeed-0.14.0.tar.gz (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 97.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting diffusers==0.22 (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for diffusers==0.22 from https://files.pythonhosted.org/packages/0c/bf/339f6c193b5bab1fc8716e3dcb87fd0f9f9a83ad2b3796f1ae0fc54c4a11/diffusers-0.22.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading diffusers-0.22.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cuda-nvrtc-cu11==11.7.99 from https://files.pythonhosted.org/packages/ef/25/922c5996aada6611b79b53985af7999fc629aee1d5d001b6a22431e18fec/nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting diffusers==0.22 (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for diffusers==0.22 from https://files.pythonhosted.org/packages/0c/bf/339f6c193b5bab1fc8716e3dcb87fd0f9f9a83ad2b3796f1ae0fc54c4a11/diffusers-0.22.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading diffusers-0.22.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cuda-nvrtc-cu11==11.7.99 from https://files.pythonhosted.org/packages/ef/25/922c5996aada6611b79b53985af7999fc629aee1d5d001b6a22431e18fec/nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cuda-runtime-cu11==11.7.99 from https://files.pythonhosted.org/packages/36/92/89cf558b514125d2ebd8344dd2f0533404b416486ff681d5434a5832a019/nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cuda-cupti-cu11==11.7.101 from https://files.pythonhosted.org/packages/e6/9d/dd0cdcd800e642e3c82ee3b5987c751afd4f3fb9cc2752517f42c3bc6e49/nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cudnn-cu11==8.5.0.96 from https://files.pythonhosted.org/packages/dc/30/66d4347d6e864334da5bb1c7571305e501dcb11b9155971421bb7bb5315f/nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cublas-cu11==11.10.3.66 from https://files.pythonhosted.org/packages/ce/41/fdeb62b5437996e841d83d7d2714ca75b886547ee8017ee2fe6ea409d983/nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cufft-cu11==10.9.0.58 from https://files.pythonhosted.org/packages/74/79/b912a77e38e41f15a0581a59f5c3548d1ddfdda3225936fb67c342719e7a/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-curand-cu11==10.2.10.91 from https://files.pythonhosted.org/packages/8f/11/af78d54b2420e64a4dd19e704f5bb69dcb5a6a3138b4465d6a48cdf59a21/nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cusolver-cu11==11.4.0.1 from https://files.pythonhosted.org/packages/3e/77/66149e3153b19312fb782ea367f3f950123b93916a45538b573fe373570a/nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-cusparse-cu11==11.7.4.91 from https://files.pythonhosted.org/packages/ea/6f/6d032cc1bb7db88a989ddce3f4968419a7edeafda362847f42f614b1f845/nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-nccl-cu11==2.14.3 from https://files.pythonhosted.org/packages/55/92/914cdb650b6a5d1478f83148597a25e90ea37d739bd563c5096b0e8a5f43/nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nvidia-nvtx-cu11==11.7.91 from https://files.pythonhosted.org/packages/23/d5/09493ff0e64fd77523afbbb075108f27a13790479efe86b9ffb4587671b5/nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for triton==2.0.0 from https://files.pythonhosted.org/packages/77/ac/28b74ec1177c730d0da8803eaff5e5025bd532bcf07cadb0fcf661abed97/triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (4.64.1)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cuda-runtime-cu11==11.7.99 from https://files.pythonhosted.org/packages/36/92/89cf558b514125d2ebd8344dd2f0533404b416486ff681d5434a5832a019/nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cuda-cupti-cu11==11.7.101 from https://files.pythonhosted.org/packages/e6/9d/dd0cdcd800e642e3c82ee3b5987c751afd4f3fb9cc2752517f42c3bc6e49/nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cudnn-cu11==8.5.0.96 from https://files.pythonhosted.org/packages/dc/30/66d4347d6e864334da5bb1c7571305e501dcb11b9155971421bb7bb5315f/nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cublas-cu11==11.10.3.66 from https://files.pythonhosted.org/packages/ce/41/fdeb62b5437996e841d83d7d2714ca75b886547ee8017ee2fe6ea409d983/nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cufft-cu11==10.9.0.58 from https://files.pythonhosted.org/packages/74/79/b912a77e38e41f15a0581a59f5c3548d1ddfdda3225936fb67c342719e7a/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-curand-cu11==10.2.10.91 from https://files.pythonhosted.org/packages/8f/11/af78d54b2420e64a4dd19e704f5bb69dcb5a6a3138b4465d6a48cdf59a21/nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cusolver-cu11==11.4.0.1 from https://files.pythonhosted.org/packages/3e/77/66149e3153b19312fb782ea367f3f950123b93916a45538b573fe373570a/nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-cusparse-cu11==11.7.4.91 from https://files.pythonhosted.org/packages/ea/6f/6d032cc1bb7db88a989ddce3f4968419a7edeafda362847f42f614b1f845/nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-nccl-cu11==2.14.3 from https://files.pythonhosted.org/packages/55/92/914cdb650b6a5d1478f83148597a25e90ea37d739bd563c5096b0e8a5f43/nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nvidia-nvtx-cu11==11.7.91 from https://files.pythonhosted.org/packages/23/d5/09493ff0e64fd77523afbbb075108f27a13790479efe86b9ffb4587671b5/nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for triton==2.0.0 from https://files.pythonhosted.org/packages/77/ac/28b74ec1177c730d0da8803eaff5e5025bd532bcf07cadb0fcf661abed97/triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from peft==0.7.0->-r requirements.txt (line 9)) (4.64.1)\u001b[0m\n",
      "\u001b[34mCollecting safetensors (from peft==0.7.0->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors from https://files.pythonhosted.org/packages/aa/9a/723fc6eed972b28bbb24241b246005093b3c27340bc8f7b7606d75a92834/safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.17.0 (from peft==0.7.0->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub>=0.17.0 from https://files.pythonhosted.org/packages/ab/28/d4b691840d73126d4c9845f8a22dad033ac872509b6d3a0d93b456eef424/huggingface_hub-0.21.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements.txt (line 12)) (6.8.0)\u001b[0m\n",
      "\u001b[35mCollecting safetensors (from peft==0.7.0->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for safetensors from https://files.pythonhosted.org/packages/aa/9a/723fc6eed972b28bbb24241b246005093b3c27340bc8f7b7606d75a92834/safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub>=0.17.0 (from peft==0.7.0->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for huggingface-hub>=0.17.0 from https://files.pythonhosted.org/packages/ab/28/d4b691840d73126d4c9845f8a22dad033ac872509b6d3a0d93b456eef424/huggingface_hub-0.21.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements.txt (line 12)) (6.8.0)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from diffusers==0.22->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/05/9e/80c20f1151432a6025690c9c2037053039b028a7b236fa81d7e7ac9dec60/regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from diffusers==0.22->-r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/05/9e/80c20f1151432a6025690c9c2037053039b028a7b236fa81d7e7ac9dec60/regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 11.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements.txt (line 12)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements.txt (line 12)) (10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (0.38.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1)) (3.24.3)\u001b[0m\n",
      "\u001b[34mCollecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading lit-18.1.2.tar.gz (161 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.0/161.0 kB 34.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 12.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements.txt (line 12)) (2.31.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements.txt (line 12)) (10.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (0.38.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1)) (3.24.3)\u001b[0m\n",
      "\u001b[35mCollecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading lit-18.1.2.tar.gz (161 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.0/161.0 kB 42.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[35mCollecting torchvision (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/95/3d/75f4926df30741f3312e7fd0988633faaf6ebf11fd5b926ddb80af96ea3e/torchvision-0.17.1-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading torchvision-0.17.1-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/4b/bb/b3e917f46610854a444bf08e6fd0fa95dd22bbfcab092e83785638daf57a/torchvision-0.17.0-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading torchvision-0.17.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting torchvision (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/95/3d/75f4926df30741f3312e7fd0988633faaf6ebf11fd5b926ddb80af96ea3e/torchvision-0.17.1-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.17.1-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/4b/bb/b3e917f46610854a444bf08e6fd0fa95dd22bbfcab092e83785638daf57a/torchvision-0.17.0-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.17.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/a3/19/2f586c109711337ba5c5b713c03d56b42a74aee966ac0bba64458dea84a1/torchvision-0.16.2-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.16.2-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/35/f8/746aab24005484577247a0caabd1cbbc9ed1e31c4fd92f3273310fd92206/torchvision-0.16.1-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.16.1-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/8b/64/00e53316beb2f19edcaa0eb283b6087cb4d5275112f24b8ea0d8b49c1d5f/torchvision-0.16.0-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.16.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/41/9e/8809e45a084680394e8d219fcf8a2c0eed2dddf1ec0a7968f4052826a6e9/torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/a3/19/2f586c109711337ba5c5b713c03d56b42a74aee966ac0bba64458dea84a1/torchvision-0.16.2-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading torchvision-0.16.2-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/35/f8/746aab24005484577247a0caabd1cbbc9ed1e31c4fd92f3273310fd92206/torchvision-0.16.1-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading torchvision-0.16.1-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/8b/64/00e53316beb2f19edcaa0eb283b6087cb4d5275112f24b8ea0d8b49c1d5f/torchvision-0.16.0-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading torchvision-0.16.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for torchvision from https://files.pythonhosted.org/packages/41/9e/8809e45a084680394e8d219fcf8a2c0eed2dddf1ec0a7968f4052826a6e9/torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers<0.19,>=0.14 (from transformers>=4.25.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/a5/bc/ec39dae3b0ea00724c0fea287091d62b0ccaa45c7a947004714e882d193d/tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.19,>=0.14 (from transformers>=4.25.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/a5/bc/ec39dae3b0ea00724c0fea287091d62b0ccaa45c7a947004714e882d193d/tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (12.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/63/93/812d78f70145c68c4e64533f4d625bea01236f27698febe15f0ceebc1566/xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (12.0.1)\u001b[0m\n",
      "\u001b[35mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (0.3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/63/93/812d78f70145c68c4e64533f4d625bea01236f27698febe15f0ceebc1566/xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (0.70.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/bf/63/bff4168e4be6da032225fe3f2492b4627bf9416a62e58b7e9dc98c6280b2/aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[35mCollecting wcwidth<0.3.0,>=0.2.12 (from ftfy->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[35mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/bf/63/bff4168e4be6da032225fe3f2492b4627bf9416a62e58b7e9dc98c6280b2/aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting wcwidth<0.3.0,>=0.2.12 (from ftfy->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/50/54/827c21cbe1187f2e39c041eeeb545f3bf327b19fadb5b97b8395f3883b25/grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/fc/b3/0c0c994fe49cd661084f8d5dc06562af53818cc0abefaca35bdc894577c3/Markdown-3.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2->-r requirements.txt (line 8)) (2.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (1.10.11)\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from deepspeed>=0.9.3->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pynvml from https://files.pythonhosted.org/packages/5b/9c/adb8070059caaa15d5a572b66bccd95900d8c1b9fa54d6ecea6ae97448d1/pynvml-11.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/70/b0/6f1ebdabfb604e39a0f84428986b89ab55f246b64cddaa495f2c953e1f6b/frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/39/a9/1f8d42c8103bcb1da6bb719f1bc018594b5acc8eae56b3fec4720ebee225/multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/69/ea/d7e961ea9b1b818a43b155ee512117be6ab9ab67c1e94967b2e64126e8e4/yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/50/54/827c21cbe1187f2e39c041eeeb545f3bf327b19fadb5b97b8395f3883b25/grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/fc/b3/0c0c994fe49cd661084f8d5dc06562af53818cc0abefaca35bdc894577c3/Markdown-3.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2->-r requirements.txt (line 8)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements.txt (line 10)) (1.10.11)\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from deepspeed>=0.9.3->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pynvml from https://files.pythonhosted.org/packages/5b/9c/adb8070059caaa15d5a572b66bccd95900d8c1b9fa54d6ecea6ae97448d1/pynvml-11.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/70/b0/6f1ebdabfb604e39a0f84428986b89ab55f246b64cddaa495f2c953e1f6b/frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/39/a9/1f8d42c8103bcb1da6bb719f1bc018594b5acc8eae56b3fec4720ebee225/multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/69/ea/d7e961ea9b1b818a43b155ee512117be6ab9ab67c1e94967b2e64126e8e4/yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata->diffusers==0.22->-r requirements.txt (line 12)) (3.16.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata->diffusers==0.22->-r requirements.txt (line 12)) (3.16.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements.txt (line 12)) (2023.7.22)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[35mDownloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\u001b[0m\n",
      "\u001b[34mDownloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 2.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.7.0-py3-none-any.whl (168 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.3/168.3 kB 24.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading diffusers-0.22.0-py3-none-any.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 86.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 5.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 100.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 71.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 87.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 2.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading peft-0.7.0-py3-none-any.whl (168 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.3/168.3 kB 38.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading diffusers-0.22.0-py3-none-any.whl (1.7 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 108.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 5.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 118.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 83.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 72.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 2.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 2.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 15.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 39.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 14.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 37.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 21.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 MB 11.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 MB 13.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 kB 24.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.3/63.3 MB 34.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 120.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 119.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 70.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.4/54.4 kB 13.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 128.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 90.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 130.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 55.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 26.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.4/773.4 kB 85.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 88.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 13.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 kB 23.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.3/63.3 MB 32.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 111.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 110.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 72.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 108.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 93.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[35mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 42.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.7/240.7 kB 36.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.8/123.8 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.3/304.3 kB 44.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.4/54.4 kB 17.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 107.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 37.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 93.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 111.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 52.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.6-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 23.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.4/773.4 kB 74.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 82.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 107.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 103.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 49.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.7/240.7 kB 42.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.8/123.8 kB 35.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.3/304.3 kB 54.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: deepspeed, lit\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, lit\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400406 sha256=879d281c3933fcf5f9df0209fb8f38928295f1301b0b3b628319f65acb5dcd3c\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/d4/cd/51/bdf520025cfe14b7ead5f59e8310a9a5383efa88bc7e6f5f02\u001b[0m\n",
      "\u001b[35mBuilding wheel for lit (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for lit (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for lit: filename=lit-18.1.2-py3-none-any.whl size=96368 sha256=2e78a6bd8c8fc223d7a7ec47a4b061b3354ddcc1cdc3c631d3780615c3030b7f\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/29/63/97/d7b62d18e2bff5fea4628ffc9f0d0296f863dea41103b00f99\u001b[0m\n",
      "\u001b[35mSuccessfully built deepspeed lit\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400407 sha256=0d49954d90ca3f1a333afddb61eb7676a6fe930451848cb44029730ed94c6e63\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/d4/cd/51/bdf520025cfe14b7ead5f59e8310a9a5383efa88bc7e6f5f02\u001b[0m\n",
      "\u001b[34mBuilding wheel for lit (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for lit (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for lit: filename=lit-18.1.2-py3-none-any.whl size=96368 sha256=2e78a6bd8c8fc223d7a7ec47a4b061b3354ddcc1cdc3c631d3780615c3030b7f\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/29/63/97/d7b62d18e2bff5fea4628ffc9f0d0296f863dea41103b00f99\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed lit\u001b[0m\n",
      "\u001b[35mInstalling collected packages: wcwidth, lit, xxhash, tensorboard-data-server, safetensors, regex, pynvml, pyarrow-hotfix, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, multidict, grpcio, ftfy, frozenlist, async-timeout, absl-py, yarl, nvidia-cusolver-cu11, nvidia-cudnn-cu11, markdown, huggingface-hub, aiosignal, tokenizers, tensorboard, diffusers, aiohttp, transformers, datasets, triton, torch, torchvision, peft, deepspeed\u001b[0m\n",
      "\u001b[35mAttempting uninstall: wcwidth\u001b[0m\n",
      "\u001b[35mFound existing installation: wcwidth 0.2.6\u001b[0m\n",
      "\u001b[35mUninstalling wcwidth-0.2.6:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled wcwidth-0.2.6\u001b[0m\n",
      "\u001b[34mInstalling collected packages: wcwidth, lit, xxhash, tensorboard-data-server, safetensors, regex, pynvml, pyarrow-hotfix, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, multidict, grpcio, ftfy, frozenlist, async-timeout, absl-py, yarl, nvidia-cusolver-cu11, nvidia-cudnn-cu11, markdown, huggingface-hub, aiosignal, tokenizers, tensorboard, diffusers, aiohttp, transformers, datasets, triton, torch, torchvision, peft, deepspeed\u001b[0m\n",
      "\u001b[34mAttempting uninstall: wcwidth\u001b[0m\n",
      "\u001b[34mFound existing installation: wcwidth 0.2.6\u001b[0m\n",
      "\u001b[34mUninstalling wcwidth-0.2.6:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled wcwidth-0.2.6\u001b[0m\n",
      "\u001b[35mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[35mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[35mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[35mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[35mFound existing installation: torch 1.13.1+cu117\u001b[0m\n",
      "\u001b[35mUninstalling torch-1.13.1+cu117:\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 1.13.1+cu117\u001b[0m\n",
      "\u001b[34mUninstalling torch-1.13.1+cu117:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled torch-1.13.1+cu117\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-1.13.1+cu117\u001b[0m\n",
      "\u001b[35mAttempting uninstall: torchvision\u001b[0m\n",
      "\u001b[35mFound existing installation: torchvision 0.14.1+cu117\u001b[0m\n",
      "\u001b[35mUninstalling torchvision-0.14.1+cu117:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled torchvision-0.14.1+cu117\u001b[0m\n",
      "\u001b[35mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[35mFound existing installation: deepspeed 0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[35mUninstalling deepspeed-0.6.1+4c3ff1a:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled deepspeed-0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torchvision\u001b[0m\n",
      "\u001b[34mFound existing installation: torchvision 0.14.1+cu117\u001b[0m\n",
      "\u001b[34mUninstalling torchvision-0.14.1+cu117:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torchvision-0.14.1+cu117\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+4c3ff1a:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+4c3ff1a\u001b[0m\n",
      "\u001b[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[35mfastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[35mtorchaudio 0.13.1+cu117 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[35mtorchdata 0.5.1 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-2.1.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 deepspeed-0.14.0 diffusers-0.22.0 frozenlist-1.4.1 ftfy-6.2.0 grpcio-1.62.1 huggingface-hub-0.21.4 lit-18.1.2 markdown-3.6 multidict-6.0.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 peft-0.7.0 pyarrow-hotfix-0.6 pynvml-11.5.0 regex-2023.12.25 safetensors-0.4.2 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.15.2 torch-2.0.1 torchvision-0.15.2 transformers-4.39.0 triton-2.0.0 wcwidth-0.2.13 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[34mtorchaudio 0.13.1+cu117 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[34mtorchdata 0.5.1 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 deepspeed-0.14.0 diffusers-0.22.0 frozenlist-1.4.1 ftfy-6.2.0 grpcio-1.62.1 huggingface-hub-0.21.4 lit-18.1.2 markdown-3.6 multidict-6.0.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 peft-0.7.0 pyarrow-hotfix-0.6 pynvml-11.5.0 regex-2023.12.25 safetensors-0.4.2 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.15.2 torch-2.0.1 torchvision-0.15.2 transformers-4.39.0 triton-2.0.0 wcwidth-0.2.13 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2024-03-22 16:29:33,814 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-03-22 16:29:33,814 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-03-22 16:29:33,924 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-03-22 16:29:34,024 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-03-22 16:29:34,126 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2024-03-22 16:29:34,142 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"t2i-acc-launch-2-2024-03-22-16-22-47-061\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-420486383638/t2i-acc-launch-2-2024-03-22-16-22-47-061/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=entry.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=entry\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-420486383638/t2i-acc-launch-2-2024-03-22-16-22-47-061/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"t2i-acc-launch-2-2024-03-22-16-22-47-061\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-420486383638/t2i-acc-launch-2-2024-03-22-16-22-47-061/source/sourcedir.tar.gz\",\"module_name\":\"entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 entry.py\u001b[0m\n",
      "\u001b[35m2024-03-22 16:29:34,169 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35m------DDD-------- yaml doc: {'compute_environment': 'LOCAL_MACHINE', 'debug': False, 'deepspeed_config': {'deepspeed_multinode_launcher': 'standard', 'gradient_accumulation_steps': 4, 'offload_optimizer_device': 'cpu', 'offload_param_device': 'cpu', 'zero3_init_flag': False, 'zero_stage': 2}, 'distributed_type': 'DEEPSPEED', 'downcast_bf16': 'no', 'machine_rank': 1, 'main_training_function': 'main', 'mixed_precision': 'fp16', 'num_machines': 2, 'num_processes': 8, 'main_process_ip': '10.2.105.242', 'main_process_port': 7777, 'rdzv_backend': 'static', 'same_network': True, 'tpu_env': [], 'tpu_use_cluster': False, 'tpu_use_sudo': False, 'use_cpu': False}\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch==2.0.1 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[35mCollecting accelerate>=0.22.0 (from -r requirements_sdxl.txt (line 2))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for accelerate>=0.22.0 from https://files.pythonhosted.org/packages/a0/11/9bfcf765e71a2c84bbf715719ba520aeacb2ad84113f14803ff1947ddf69/accelerate-0.28.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 3)) (0.15.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: transformers>=4.25.1 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 4)) (4.39.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ftfy in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 5)) (6.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 6)) (2.16.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 7)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 8)) (2.18.0)\u001b[0m\n",
      "\u001b[35mCollecting xformers (from -r requirements_sdxl.txt (line 9))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/ba/74/5b815b33be1320318dd676a8980a3cc627494a60c5674f8b2efd64e12c2e/xformers-0.0.25-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.25-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34m2024-03-22 16:29:34,622 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-22 16:29:34,622 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-22 16:29:34,730 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-22 16:29:34,832 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-22 16:29:34,934 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-22 16:29:34,951 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"t2i-acc-launch-2-2024-03-22-16-22-47-061\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-420486383638/t2i-acc-launch-2-2024-03-22-16-22-47-061/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-420486383638/t2i-acc-launch-2-2024-03-22-16-22-47-061/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"t2i-acc-launch-2-2024-03-22-16-22-47-061\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-420486383638/t2i-acc-launch-2-2024-03-22-16-22-47-061/source/sourcedir.tar.gz\",\"module_name\":\"entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 entry.py\u001b[0m\n",
      "\u001b[34m2024-03-22 16:29:34,976 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m------DDD-------- yaml doc: {'compute_environment': 'LOCAL_MACHINE', 'debug': False, 'deepspeed_config': {'deepspeed_multinode_launcher': 'standard', 'gradient_accumulation_steps': 4, 'offload_optimizer_device': 'cpu', 'offload_param_device': 'cpu', 'zero3_init_flag': False, 'zero_stage': 2}, 'distributed_type': 'DEEPSPEED', 'downcast_bf16': 'no', 'machine_rank': 0, 'main_training_function': 'main', 'mixed_precision': 'fp16', 'num_machines': 2, 'num_processes': 8, 'main_process_ip': '10.2.105.242', 'main_process_port': 7777, 'rdzv_backend': 'static', 'same_network': True, 'tpu_env': [], 'tpu_use_cluster': False, 'tpu_use_sudo': False, 'use_cpu': False}\u001b[0m\n",
      "\u001b[35mCollecting wandb (from -r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for wandb from https://files.pythonhosted.org/packages/97/c9/5af930be59487ab6c7e76dc1ffa31f7f035be2e50cb17cc56b87ba47ff8f/wandb-0.16.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading wandb-0.16.4-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes (from -r requirements_sdxl.txt (line 11))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/df/f7/da7090dc7690d6a69a311cfb026825ddd9d3693c9c03dd47fdb56521ca41/bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: deepspeed>=0.9.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 12)) (0.14.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: diffusers==0.22 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 13)) (0.22.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.99)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.99)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.101)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (8.5.0.96)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.10.3.66)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (10.9.0.58)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (10.2.10.91)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.4.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.4.91)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (2.14.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.91)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (6.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (0.21.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (2023.12.25)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (2.31.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (0.4.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (10.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (0.38.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (3.24.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: lit in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (18.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.22.0->-r requirements_sdxl.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.22.0->-r requirements_sdxl.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.22.0->-r requirements_sdxl.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.25.1->-r requirements_sdxl.txt (line 4)) (0.15.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.25.1->-r requirements_sdxl.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.9/site-packages (from ftfy->-r requirements_sdxl.txt (line 5)) (0.2.13)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (2.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (1.62.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (3.20.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (0.7.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (2.3.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2->-r requirements_sdxl.txt (line 7)) (2.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (12.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (0.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (0.3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (2.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (0.70.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (2023.6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (3.9.3)\u001b[0m\n",
      "\u001b[35mINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[35mCollecting xformers (from -r requirements_sdxl.txt (line 9))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/ad/9b/73bc3d71625405533d359c0601d3d6f2dc94207c43d1c946dffb96a666b9/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==2.0.1 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.22.0 (from -r requirements_sdxl.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for accelerate>=0.22.0 from https://files.pythonhosted.org/packages/a0/11/9bfcf765e71a2c84bbf715719ba520aeacb2ad84113f14803ff1947ddf69/accelerate-0.28.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 3)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers>=4.25.1 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 4)) (4.39.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ftfy in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 5)) (6.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 6)) (2.16.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 7)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 8)) (2.18.0)\u001b[0m\n",
      "\u001b[34mCollecting xformers (from -r requirements_sdxl.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/ba/74/5b815b33be1320318dd676a8980a3cc627494a60c5674f8b2efd64e12c2e/xformers-0.0.25-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.25-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for wandb from https://files.pythonhosted.org/packages/97/c9/5af930be59487ab6c7e76dc1ffa31f7f035be2e50cb17cc56b87ba47ff8f/wandb-0.16.4-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.16.4-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements_sdxl.txt (line 11))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/df/f7/da7090dc7690d6a69a311cfb026825ddd9d3693c9c03dd47fdb56521ca41/bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed>=0.9.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 12)) (0.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: diffusers==0.22 in /opt/conda/lib/python3.9/site-packages (from -r requirements_sdxl.txt (line 13)) (0.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.101)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (8.5.0.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.10.3.66)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (10.9.0.58)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (10.2.10.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.4.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (2.14.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (11.7.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.9/site-packages (from torch==2.0.1->-r requirements_sdxl.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (6.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (0.21.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (from diffusers==0.22->-r requirements_sdxl.txt (line 13)) (10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (0.38.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (3.24.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lit in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (18.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.22.0->-r requirements_sdxl.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.22.0->-r requirements_sdxl.txt (line 2)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.22.0->-r requirements_sdxl.txt (line 2)) (6.0.1)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/09/60/4fb406b282a8ee7a0e1f35a1080e9d424207d7bc6846723c70b0d5c858ce/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/8d/6f/777a1120535ec6e71ae7934ff228dfac4a9969bfd5e136268fd315b7e32f/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/c6/20/a4a35698012a319daa81542777493ff0f3e9041a744eaececdd19db672c4/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/93/a9/973dd417531dc29eff35e0a3e716039ed7c739ec3846620d2d5e70c9ea27/xformers-0.0.22-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.22-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements_sdxl.txt (line 10)) (8.1.2)\u001b[0m\n",
      "\u001b[35mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/67/c7/995360c87dd74e27539ccbfecddfb58e08f140d849fcd7f35d2ed1a5f80f/GitPython-3.1.42-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/26/89/9b786c95b202b2da0d3a481497e79447cff6c753b4f0af5a3aecf3271a67/sentry_sdk-1.43.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading sentry_sdk-1.43.0-py2.py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for docker-pycreds>=0.4.0 from https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting setproctitle (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/c3/9c/0ff2dc4967c30fe1e60ef5d651547065e431380f8b30af0e8fafadcb2a26/setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting appdirs>=1.4.3 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for appdirs>=1.4.3 from https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (1.10.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pynvml in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (11.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (1.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (23.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (1.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (6.0.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (1.9.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (4.0.3)\u001b[0m\n",
      "\u001b[35mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (3.16.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.25.1->-r requirements_sdxl.txt (line 4)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.25.1->-r requirements_sdxl.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.9/site-packages (from ftfy->-r requirements_sdxl.txt (line 5)) (0.2.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (1.62.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (0.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements_sdxl.txt (line 6)) (2.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2->-r requirements_sdxl.txt (line 7)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (12.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->-r requirements_sdxl.txt (line 8)) (3.9.3)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting xformers (from -r requirements_sdxl.txt (line 9))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/ad/9b/73bc3d71625405533d359c0601d3d6f2dc94207c43d1c946dffb96a666b9/xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.24-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/09/60/4fb406b282a8ee7a0e1f35a1080e9d424207d7bc6846723c70b0d5c858ce/xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.23.post1-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/8d/6f/777a1120535ec6e71ae7934ff228dfac4a9969bfd5e136268fd315b7e32f/xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.23-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/c6/20/a4a35698012a319daa81542777493ff0f3e9041a744eaececdd19db672c4/xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.22.post7-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/93/a9/973dd417531dc29eff35e0a3e716039ed7c739ec3846620d2d5e70c9ea27/xformers-0.0.22-cp39-cp39-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.22-cp39-cp39-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements_sdxl.txt (line 10)) (8.1.2)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/67/c7/995360c87dd74e27539ccbfecddfb58e08f140d849fcd7f35d2ed1a5f80f/GitPython-3.1.42-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/26/89/9b786c95b202b2da0d3a481497e79447cff6c753b4f0af5a3aecf3271a67/sentry_sdk-1.43.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.43.0-py2.py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for docker-pycreds>=0.4.0 from https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/c3/9c/0ff2dc4967c30fe1e60ef5d651547065e431380f8b30af0e8fafadcb2a26/setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3 (from wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for appdirs>=1.4.3 from https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (1.10.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pynvml in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.9.3->-r requirements_sdxl.txt (line 12)) (11.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements_sdxl.txt (line 8)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements_sdxl.txt (line 8)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements_sdxl.txt (line 8)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[35mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[35mDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 48.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.22-cp39-cp39-manylinux2014_x86_64.whl (211.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements_sdxl.txt (line 8)) (4.0.3)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (3.16.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->diffusers==0.22->-r requirements_sdxl.txt (line 13)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements_sdxl.txt (line 8)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements_sdxl.txt (line 8)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->-r requirements_sdxl.txt (line 8)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch==2.0.1->-r requirements_sdxl.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements_sdxl.txt (line 10))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 32.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.22-cp39-cp39-manylinux2014_x86_64.whl (211.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.6/211.6 MB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 101.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.6/211.6 MB 11.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 91.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 MB 25.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[35mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[35mDownloading GitPython-3.1.42-py3-none-any.whl (195 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.4/195.4 kB 51.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading sentry_sdk-1.43.0-py2.py3-none-any.whl (264 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 kB 54.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[35mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 22.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 MB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.42-py3-none-any.whl (195 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.4/195.4 kB 4.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.43.0-py2.py3-none-any.whl (264 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 kB 5.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 1.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb, xformers, bitsandbytes, accelerate\u001b[0m\n",
      "\u001b[34mInstalling collected packages: appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb, xformers, bitsandbytes, accelerate\u001b[0m\n",
      "\u001b[35mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[35mFound existing installation: accelerate 0.21.0\u001b[0m\n",
      "\u001b[35mUninstalling accelerate-0.21.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled accelerate-0.21.0\u001b[0m\n",
      "\u001b[35mSuccessfully installed GitPython-3.1.42 accelerate-0.28.0 appdirs-1.4.4 bitsandbytes-0.43.0 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.43.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4 xformers-0.0.22\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.21.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.21.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.21.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.42 accelerate-0.28.0 appdirs-1.4.4 bitsandbytes-0.43.0 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.43.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4 xformers-0.0.22\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mW&B disabled.\u001b[0m\n",
      "\u001b[34mW&B disabled.\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:57,114] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:57,105] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:57,142] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:57,143] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:57,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:57,155] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:57,157] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:57,158] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:59,137] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:59,113] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:59,152] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:59,157] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:29:59,169] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[35mNum processes: 8\u001b[0m\n",
      "\u001b[35mProcess index: 6\u001b[0m\n",
      "\u001b[35mLocal process index: 2\u001b[0m\n",
      "\u001b[35mDevice: cuda:2\u001b[0m\n",
      "\u001b[35mMixed precision type: fp16\u001b[0m\n",
      "\u001b[35mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[35m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[35mNum processes: 8\u001b[0m\n",
      "\u001b[35mProcess index: 5\u001b[0m\n",
      "\u001b[35mLocal process index: 1\u001b[0m\n",
      "\u001b[35mDevice: cuda:1\u001b[0m\n",
      "\u001b[35mMixed precision type: fp16\u001b[0m\n",
      "\u001b[35mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[35m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[35mNum processes: 8\u001b[0m\n",
      "\u001b[35mProcess index: 7\u001b[0m\n",
      "\u001b[35mLocal process index: 3\u001b[0m\n",
      "\u001b[35mDevice: cuda:3\u001b[0m\n",
      "\u001b[35mMixed precision type: fp16\u001b[0m\n",
      "\u001b[35mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[35m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[35mNum processes: 8\u001b[0m\n",
      "\u001b[35mProcess index: 4\u001b[0m\n",
      "\u001b[35mLocal process index: 0\u001b[0m\n",
      "\u001b[35mDevice: cuda:0\u001b[0m\n",
      "\u001b[35mMixed precision type: fp16\u001b[0m\n",
      "\u001b[35mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:59,177] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:59,184] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:59,206] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:29:59,206] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[34mNum processes: 8\u001b[0m\n",
      "\u001b[34mProcess index: 2\u001b[0m\n",
      "\u001b[34mLocal process index: 2\u001b[0m\n",
      "\u001b[34mDevice: cuda:2\u001b[0m\n",
      "\u001b[34mMixed precision type: fp16\u001b[0m\n",
      "\u001b[34mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[34mNum processes: 8\u001b[0m\n",
      "\u001b[34mProcess index: 0\u001b[0m\n",
      "\u001b[34mLocal process index: 0\u001b[0m\n",
      "\u001b[34mDevice: cuda:0\u001b[0m\n",
      "\u001b[34mMixed precision type: fp16\u001b[0m\n",
      "\u001b[34mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[34mNum processes: 8\u001b[0m\n",
      "\u001b[34mProcess index: 3\u001b[0m\n",
      "\u001b[34mLocal process index: 3\u001b[0m\n",
      "\u001b[34mDevice: cuda:3\u001b[0m\n",
      "\u001b[34mMixed precision type: fp16\u001b[0m\n",
      "\u001b[34mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m03/22/2024 16:29:59 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl\u001b[0m\n",
      "\u001b[34mNum processes: 8\u001b[0m\n",
      "\u001b[34mProcess index: 1\u001b[0m\n",
      "\u001b[34mLocal process index: 1\u001b[0m\n",
      "\u001b[34mDevice: cuda:1\u001b[0m\n",
      "\u001b[34mMixed precision type: fp16\u001b[0m\n",
      "\u001b[34mds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 4, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'nvme_path': None}, 'offload_param': {'device': 'cpu', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}, 'bf16': {'enabled': False}}\u001b[0m\n",
      "\u001b[35mYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[35mYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[35m{'clip_sample_range', 'thresholding', 'dynamic_thresholding_ratio', 'variance_type'} was not found in config. Values will be initialized to default values.\u001b[0m\n",
      "\u001b[34mYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[34mYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\u001b[0m\n",
      "\u001b[34m{'clip_sample_range', 'dynamic_thresholding_ratio', 'thresholding', 'variance_type'} was not found in config. Values will be initialized to default values.\u001b[0m\n",
      "\u001b[35m{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 833 examples [00:00, 11359.99 examples/s]\u001b[0m\n",
      "\u001b[34m{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 833 examples [00:00, 11504.83 examples/s]\u001b[0m\n",
      "\u001b[34malgo-1:425:425 [0] NCCL INFO Bootstrap : Using eth0:10.2.105.242<0>\u001b[0m\n",
      "\u001b[34malgo-1:425:425 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:425:425 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:425:425 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:428:428 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:426:426 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:427:427 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:428:428 [3] NCCL INFO Bootstrap : Using eth0:10.2.105.242<0>\u001b[0m\n",
      "\u001b[34malgo-1:428:428 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:428:428 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:426:426 [1] NCCL INFO Bootstrap : Using eth0:10.2.105.242<0>\u001b[0m\n",
      "\u001b[34malgo-1:426:426 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:426:426 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:427:427 [2] NCCL INFO Bootstrap : Using eth0:10.2.105.242<0>\u001b[0m\n",
      "\u001b[34malgo-1:427:427 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:427:427 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/-1/-1->0->4\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:757 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:428:755 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 00/0 : 7[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:428:755 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:425:757 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 01/0 : 7[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:425:425 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:424:424 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:427:427 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:426:426 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[35malgo-2:425:425 [1] NCCL INFO Bootstrap : Using eth0:10.2.70.52<0>\u001b[0m\n",
      "\u001b[35malgo-2:425:425 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:425:425 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35malgo-2:424:424 [0] NCCL INFO Bootstrap : Using eth0:10.2.70.52<0>\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:424:424 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:424:424 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35malgo-2:426:426 [2] NCCL INFO Bootstrap : Using eth0:10.2.70.52<0>\u001b[0m\n",
      "\u001b[35malgo-2:426:426 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:426:426 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35malgo-2:427:427 [3] NCCL INFO Bootstrap : Using eth0:10.2.70.52<0>\u001b[0m\n",
      "\u001b[35malgo-2:427:427 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[35malgo-2:427:427 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 2\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] 5/0/-1->4->-1\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:753 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Channel 00/0 : 7[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:427:751 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Channel 01/0 : 7[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:427:751 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:753 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:424:753 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:753 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:753 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:753 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:425:746 [1] NCCL INFO comm 0x55ebd3ab7540 rank 5 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:427:749 [3] NCCL INFO comm 0x55b858320090 rank 7 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:426:748 [2] NCCL INFO comm 0x56333186e910 rank 6 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:424:747 [0] NCCL INFO comm 0x558d823df330 rank 4 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:425:757 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:757 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:757 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:757 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:427:754 [2] NCCL INFO comm 0x55f09ddbf040 rank 2 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:426:753 [1] NCCL INFO comm 0x559b2538a360 rank 1 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:428:752 [3] NCCL INFO comm 0x562634a4bb10 rank 3 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:425:751 [0] NCCL INFO comm 0x55de31224960 rank 0 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[35m[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.268166303634644 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 27.79963207244873 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 27.944374084472656 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 27.906339645385742 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:24,279] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[35m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[35m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[35m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.32281494140625 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.340604305267334 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.32462453842163 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 30.434765338897705 seconds\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:27 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[35mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[35mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[35mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[35mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[35mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[35mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[35mAdam Optimizer #0 is created with AVX2 arithmetic capability.\u001b[0m\n",
      "\u001b[35mConfig: alpha=0.000001, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:27 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:27 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:27 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:29 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] 5/0/-1->4->-1\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:424:959 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Channel 00/0 : 7[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:427:960 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:424:959 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 00 : 4[1b0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 01 : 4[1b0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Channel 01/0 : 7[1e0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:427:960 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Channel 00 : 5[1c0] -> 4[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Channel 01 : 5[1c0] -> 4[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:424:959 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:959 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:959 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[35malgo-2:424:959 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[35malgo-2:425:955 [1] NCCL INFO comm 0x55ec26fef5a0 rank 5 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:426:958 [2] NCCL INFO comm 0x563340054670 rank 6 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:30 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:00:1c.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/-1/-1->0->4\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:425:968 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 00/0 : 7[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Channel 00 : 1[1c0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Channel 00 : 2[1d0] -> 3[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Channel 01 : 1[1c0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Channel 01 : 2[1d0] -> 3[1e0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Channel 00/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:428:965 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:425:968 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 01/0 : 7[1e0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 00 : 0[1b0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 01 : 0[1b0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Channel 00 : 1[1c0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Could not enable P2P between dev 1(=1c0) and dev 0(=1b0)\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Channel 01 : 1[1c0] -> 0[1b0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Channel 01/0 : 3[1e0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:428:965 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 3(=1e0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Channel 00 : 3[1e0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Could not enable P2P between dev 3(=1e0) and dev 2(=1d0)\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Channel 01 : 3[1e0] -> 2[1d0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Channel 00 : 2[1d0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Could not enable P2P between dev 2(=1d0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Channel 01 : 2[1d0] -> 1[1c0] via SHM/direct/direct\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:425:968 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 00/0 : 4[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:968 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 01/0 : 4[1b0] -> 0[1b0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 00/0 : 0[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:968 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Channel 01/0 : 0[1b0] -> 4[1b0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34malgo-1:425:968 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:00:1b.0 path /sys/devices/pci0000:00/\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Could not enable P2P between dev 0(=1b0) and dev 1(=1c0)\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:426:962 [1] NCCL INFO comm 0x559b2e60ac10 rank 1 nranks 8 cudaDev 1 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:428:963 [3] NCCL INFO comm 0x5626b6cbe3a0 rank 3 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:427:964 [2] NCCL INFO comm 0x55f0f15567f0 rank 2 nranks 8 cudaDev 2 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:425:961 [0] NCCL INFO comm 0x55de3e380230 rank 0 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:427:957 [3] NCCL INFO comm 0x55b8698970f0 rank 7 nranks 8 cudaDev 3 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[35malgo-2:424:956 [0] NCCL INFO comm 0x558dd4b13010 rank 4 nranks 8 cudaDev 0 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,381] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,385] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,385] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,810] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,810] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,810] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,810] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500,000,000\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,810] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500,000,000\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,810] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:31,810] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:36,794] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:36,795] [INFO] [utils.py:801:see_memory_usage] MA 6.69 GB         Max_MA 6.69 GB         CA 6.83 GB         Max_CA 7 GB\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:36,795] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 56.81 GB, percent = 30.4%\u001b[0m\n",
      "\u001b[35mSteps:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,241] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,242] [INFO] [utils.py:801:see_memory_usage] MA 6.69 GB         Max_MA 6.69 GB         CA 6.83 GB         Max_CA 7 GB\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,242] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.02 GB, percent = 31.6%\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,243] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,396] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,397] [INFO] [utils.py:801:see_memory_usage] MA 6.69 GB         Max_MA 6.69 GB         CA 6.83 GB         Max_CA 7 GB\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,397] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 59.97 GB, percent = 32.1%\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,408] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,408] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,409] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-06], mom=[(0.9, 0.999)]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,414] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,414] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,414] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,414] [INFO] [config.py:1000:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,414] [INFO] [config.py:1000:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,414] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,414] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd61df61520>\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   fp16_auto_cast ............... True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 4\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   graph_harvesting ............. False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,415] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  2\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,416] [INFO] [config.py:1000:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,417] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,417] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:37,417] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": null\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": null\n",
      "        }, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": false\n",
      "    }, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:37 - INFO - __main__ - ***** Running training *****\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:37 - INFO - __main__ -   Num examples = 833\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:37 - INFO - __main__ -   Num Epochs = 8\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:37 - INFO - __main__ -   Instantaneous batch size per device = 2\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:37 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:37 - INFO - __main__ -   Gradient Accumulation steps = 4\u001b[0m\n",
      "\u001b[34m03/22/2024 16:31:37 - INFO - __main__ -   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mSteps:   0%|          | 0/100 [00:06<?, ?it/s, lr=1e-6, step_loss=0.0196]\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 0/100 [00:06<?, ?it/s, lr=1e-6, step_loss=0.0412]\u001b[0m\n",
      "\u001b[35mSteps:   0%|          | 0/100 [00:11<?, ?it/s, lr=1e-6, step_loss=0.0288]\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 0/100 [00:10<?, ?it/s, lr=1e-6, step_loss=0.045]\u001b[0m\n",
      "\u001b[35mSteps:   0%|          | 0/100 [00:15<?, ?it/s, lr=1e-6, step_loss=0.0133]\u001b[0m\n",
      "\u001b[34mSteps:   0%|          | 0/100 [00:15<?, ?it/s, lr=1e-6, step_loss=0.028]\u001b[0m\n",
      "\u001b[35mSteps:   1%|          | 1/100 [00:22<37:30, 22.74s/it, lr=1e-6, step_loss=0.0133]\u001b[0m\n",
      "\u001b[35mSteps:   1%|          | 1/100 [00:22<37:30, 22.74s/it, lr=1e-6, step_loss=0.00486]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:31:59,566] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 1/100 [00:22<36:27, 22.10s/it, lr=1e-6, step_loss=0.028]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 1/100 [00:22<36:27, 22.10s/it, lr=1e-6, step_loss=0.0279]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 1/100 [00:26<36:27, 22.10s/it, lr=1e-6, step_loss=0.0525]\u001b[0m\n",
      "\u001b[35mSteps:   1%|          | 1/100 [00:27<37:30, 22.74s/it, lr=1e-6, step_loss=0.00782]\u001b[0m\n",
      "\u001b[35mSteps:   1%|          | 1/100 [00:31<37:30, 22.74s/it, lr=1e-6, step_loss=0.084]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 1/100 [00:30<36:27, 22.10s/it, lr=1e-6, step_loss=0.127]\u001b[0m\n",
      "\u001b[34mSteps:   1%|          | 1/100 [00:35<36:27, 22.10s/it, lr=1e-6, step_loss=0.0879]\u001b[0m\n",
      "\u001b[35mSteps:   1%|          | 1/100 [00:36<37:30, 22.74s/it, lr=1e-6, step_loss=0.00913]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:32:18,025] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 2/100 [00:40<32:35, 19.96s/it, lr=1e-6, step_loss=0.0879]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 2/100 [00:40<32:35, 19.96s/it, lr=1e-6, step_loss=0.00909]\u001b[0m\n",
      "\u001b[35mSteps:   2%|▏         | 2/100 [00:41<33:01, 20.22s/it, lr=1e-6, step_loss=0.00913]\u001b[0m\n",
      "\u001b[35mSteps:   2%|▏         | 2/100 [00:41<33:01, 20.22s/it, lr=1e-6, step_loss=0.0519]\u001b[0m\n",
      "\u001b[35mSteps:   2%|▏         | 2/100 [00:45<33:01, 20.22s/it, lr=1e-6, step_loss=0.0253]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 2/100 [00:44<32:35, 19.96s/it, lr=1e-6, step_loss=0.0168]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 2/100 [00:49<32:35, 19.96s/it, lr=1e-6, step_loss=0.0103]\u001b[0m\n",
      "\u001b[35mSteps:   2%|▏         | 2/100 [00:50<33:01, 20.22s/it, lr=1e-6, step_loss=0.103]\u001b[0m\n",
      "\u001b[35mSteps:   2%|▏         | 2/100 [00:54<33:01, 20.22s/it, lr=1e-6, step_loss=0.0719]\u001b[0m\n",
      "\u001b[34mSteps:   2%|▏         | 2/100 [00:54<32:35, 19.96s/it, lr=1e-6, step_loss=0.0804]\u001b[0m\n",
      "\u001b[35mSteps:   3%|▎         | 3/100 [00:59<31:27, 19.46s/it, lr=1e-6, step_loss=0.0719]\u001b[0m\n",
      "\u001b[35mSteps:   3%|▎         | 3/100 [00:59<31:27, 19.46s/it, lr=1e-6, step_loss=0.00988]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:32:36,581] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 3/100 [00:59<31:13, 19.32s/it, lr=1e-6, step_loss=0.0804]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 3/100 [00:59<31:13, 19.32s/it, lr=1e-6, step_loss=0.00685]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 3/100 [01:03<31:13, 19.32s/it, lr=1e-6, step_loss=0.00385]\u001b[0m\n",
      "\u001b[35mSteps:   3%|▎         | 3/100 [01:04<31:27, 19.46s/it, lr=1e-6, step_loss=0.0971]\u001b[0m\n",
      "\u001b[35mSteps:   3%|▎         | 3/100 [01:08<31:27, 19.46s/it, lr=1e-6, step_loss=0.0357]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 3/100 [01:08<31:13, 19.32s/it, lr=1e-6, step_loss=0.0236]\u001b[0m\n",
      "\u001b[34mSteps:   3%|▎         | 3/100 [01:12<31:13, 19.32s/it, lr=1e-6, step_loss=0.0755]\u001b[0m\n",
      "\u001b[35mSteps:   3%|▎         | 3/100 [01:13<31:27, 19.46s/it, lr=1e-6, step_loss=0.0663]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:32:55,136] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 4/100 [01:17<30:25, 19.02s/it, lr=1e-6, step_loss=0.0755]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 4/100 [01:17<30:25, 19.02s/it, lr=1e-6, step_loss=0.0482]\u001b[0m\n",
      "\u001b[35mSteps:   4%|▍         | 4/100 [01:18<30:33, 19.10s/it, lr=1e-6, step_loss=0.0663]\u001b[0m\n",
      "\u001b[35mSteps:   4%|▍         | 4/100 [01:18<30:33, 19.10s/it, lr=1e-6, step_loss=0.0153]\u001b[0m\n",
      "\u001b[35mSteps:   4%|▍         | 4/100 [01:22<30:33, 19.10s/it, lr=1e-6, step_loss=0.0381]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 4/100 [01:22<30:25, 19.02s/it, lr=1e-6, step_loss=0.073]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 4/100 [01:26<30:25, 19.02s/it, lr=1e-6, step_loss=0.0491]\u001b[0m\n",
      "\u001b[35mSteps:   4%|▍         | 4/100 [01:27<30:33, 19.10s/it, lr=1e-6, step_loss=0.0615]\u001b[0m\n",
      "\u001b[35mSteps:   4%|▍         | 4/100 [01:31<30:33, 19.10s/it, lr=1e-6, step_loss=0.0509]\u001b[0m\n",
      "\u001b[34mSteps:   4%|▍         | 4/100 [01:31<30:25, 19.02s/it, lr=1e-6, step_loss=0.041]\u001b[0m\n",
      "\u001b[35mSteps:   5%|▌         | 5/100 [01:36<29:54, 18.89s/it, lr=1e-6, step_loss=0.0509]\u001b[0m\n",
      "\u001b[35mSteps:   5%|▌         | 5/100 [01:36<29:54, 18.89s/it, lr=1e-6, step_loss=0.0172]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:33:13,638] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 5/100 [01:36<29:48, 18.83s/it, lr=1e-6, step_loss=0.041]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 5/100 [01:36<29:48, 18.83s/it, lr=1e-6, step_loss=0.0367]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 5/100 [01:40<29:48, 18.83s/it, lr=1e-6, step_loss=0.037]\u001b[0m\n",
      "\u001b[35mSteps:   5%|▌         | 5/100 [01:41<29:54, 18.89s/it, lr=1e-6, step_loss=0.0701]\u001b[0m\n",
      "\u001b[35mSteps:   5%|▌         | 5/100 [01:45<29:54, 18.89s/it, lr=1e-6, step_loss=0.048]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 5/100 [01:45<29:48, 18.83s/it, lr=1e-6, step_loss=0.0703]\u001b[0m\n",
      "\u001b[34mSteps:   5%|▌         | 5/100 [01:49<29:48, 18.83s/it, lr=1e-6, step_loss=0.0227]\u001b[0m\n",
      "\u001b[35mSteps:   5%|▌         | 5/100 [01:50<29:54, 18.89s/it, lr=1e-6, step_loss=0.0356]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:33:32,129] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 6/100 [01:54<29:19, 18.72s/it, lr=1e-6, step_loss=0.0227]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 6/100 [01:54<29:19, 18.72s/it, lr=1e-6, step_loss=0.0496]\u001b[0m\n",
      "\u001b[35mSteps:   6%|▌         | 6/100 [01:55<29:22, 18.75s/it, lr=1e-6, step_loss=0.0356]\u001b[0m\n",
      "\u001b[35mSteps:   6%|▌         | 6/100 [01:55<29:22, 18.75s/it, lr=1e-6, step_loss=0.0588]\u001b[0m\n",
      "\u001b[35mSteps:   6%|▌         | 6/100 [01:59<29:22, 18.75s/it, lr=1e-6, step_loss=0.0744]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 6/100 [01:59<29:19, 18.72s/it, lr=1e-6, step_loss=0.116]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 6/100 [02:03<29:19, 18.72s/it, lr=1e-6, step_loss=0.0518]\u001b[0m\n",
      "\u001b[35mSteps:   6%|▌         | 6/100 [02:04<29:22, 18.75s/it, lr=1e-6, step_loss=0.0275]\u001b[0m\n",
      "\u001b[35mSteps:   6%|▌         | 6/100 [02:08<29:22, 18.75s/it, lr=1e-6, step_loss=0.0857]\u001b[0m\n",
      "\u001b[34mSteps:   6%|▌         | 6/100 [02:08<29:19, 18.72s/it, lr=1e-6, step_loss=0.0704]\u001b[0m\n",
      "\u001b[35mSteps:   7%|▋         | 7/100 [02:13<28:59, 18.70s/it, lr=1e-6, step_loss=0.0857]\u001b[0m\n",
      "\u001b[35mSteps:   7%|▋         | 7/100 [02:13<28:59, 18.70s/it, lr=1e-6, step_loss=0.0678]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:33:50,720] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 7/100 [02:13<28:56, 18.67s/it, lr=1e-6, step_loss=0.0704]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 7/100 [02:13<28:56, 18.67s/it, lr=1e-6, step_loss=0.0512]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 7/100 [02:17<28:56, 18.67s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[35mSteps:   7%|▋         | 7/100 [02:18<28:59, 18.70s/it, lr=1e-6, step_loss=0.0819]\u001b[0m\n",
      "\u001b[35mSteps:   7%|▋         | 7/100 [02:22<28:59, 18.70s/it, lr=1e-6, step_loss=0.102]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 7/100 [02:22<28:56, 18.67s/it, lr=1e-6, step_loss=0.0321]\u001b[0m\n",
      "\u001b[34mSteps:   7%|▋         | 7/100 [02:26<28:56, 18.67s/it, lr=1e-6, step_loss=0.0134]\u001b[0m\n",
      "\u001b[35mSteps:   7%|▋         | 7/100 [02:27<28:59, 18.70s/it, lr=1e-6, step_loss=0.0701]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:34:09,210] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 8/100 [02:31<28:32, 18.62s/it, lr=1e-6, step_loss=0.0134]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 8/100 [02:31<28:32, 18.62s/it, lr=1e-6, step_loss=0.06]\u001b[0m\n",
      "\u001b[35mSteps:   8%|▊         | 8/100 [02:32<28:34, 18.63s/it, lr=1e-6, step_loss=0.0701]\u001b[0m\n",
      "\u001b[35mSteps:   8%|▊         | 8/100 [02:32<28:34, 18.63s/it, lr=1e-6, step_loss=0.0221]\u001b[0m\n",
      "\u001b[35mSteps:   8%|▊         | 8/100 [02:36<28:34, 18.63s/it, lr=1e-6, step_loss=0.047]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 8/100 [02:36<28:32, 18.62s/it, lr=1e-6, step_loss=0.0212]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 8/100 [02:40<28:32, 18.62s/it, lr=1e-6, step_loss=0.0961]\u001b[0m\n",
      "\u001b[35mSteps:   8%|▊         | 8/100 [02:41<28:34, 18.63s/it, lr=1e-6, step_loss=0.0756]\u001b[0m\n",
      "\u001b[35mSteps:   8%|▊         | 8/100 [02:45<28:34, 18.63s/it, lr=1e-6, step_loss=0.0193]\u001b[0m\n",
      "\u001b[34mSteps:   8%|▊         | 8/100 [02:45<28:32, 18.62s/it, lr=1e-6, step_loss=0.0675]\u001b[0m\n",
      "\u001b[35mSteps:   9%|▉         | 9/100 [02:50<28:12, 18.60s/it, lr=1e-6, step_loss=0.0193]\u001b[0m\n",
      "\u001b[35mSteps:   9%|▉         | 9/100 [02:50<28:12, 18.60s/it, lr=1e-6, step_loss=0.0826]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:34:27,736] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 9/100 [02:50<28:11, 18.59s/it, lr=1e-6, step_loss=0.0675]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 9/100 [02:50<28:11, 18.59s/it, lr=1e-6, step_loss=0.0251]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 9/100 [02:54<28:11, 18.59s/it, lr=1e-6, step_loss=0.0433]\u001b[0m\n",
      "\u001b[35mSteps:   9%|▉         | 9/100 [02:55<28:12, 18.60s/it, lr=1e-6, step_loss=0.102]\u001b[0m\n",
      "\u001b[35mSteps:   9%|▉         | 9/100 [02:59<28:12, 18.60s/it, lr=1e-6, step_loss=0.0203]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 9/100 [02:59<28:11, 18.59s/it, lr=1e-6, step_loss=0.0714]\u001b[0m\n",
      "\u001b[34mSteps:   9%|▉         | 9/100 [03:03<28:11, 18.59s/it, lr=1e-6, step_loss=0.0202]\u001b[0m\n",
      "\u001b[35mSteps:   9%|▉         | 9/100 [03:04<28:12, 18.60s/it, lr=1e-6, step_loss=0.0975]\u001b[0m\n",
      "\u001b[35mSteps:  10%|█         | 10/100 [03:09<27:51, 18.57s/it, lr=1e-6, step_loss=0.0975]\u001b[0m\n",
      "\u001b[35mSteps:  10%|█         | 10/100 [03:09<27:51, 18.57s/it, lr=1e-6, step_loss=0.0746]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:34:46,255] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 10/100 [03:08<27:50, 18.57s/it, lr=1e-6, step_loss=0.0202]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 10/100 [03:08<27:50, 18.57s/it, lr=1e-6, step_loss=0.0445]\u001b[0m\n",
      "\u001b[35mSteps:  10%|█         | 10/100 [03:13<27:51, 18.57s/it, lr=1e-6, step_loss=0.0996]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 10/100 [03:13<27:50, 18.57s/it, lr=1e-6, step_loss=0.0193]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 10/100 [03:17<27:50, 18.57s/it, lr=1e-6, step_loss=0.0702]\u001b[0m\n",
      "\u001b[35mSteps:  10%|█         | 10/100 [03:18<27:51, 18.57s/it, lr=1e-6, step_loss=0.0346]\u001b[0m\n",
      "\u001b[35mSteps:  10%|█         | 10/100 [03:22<27:51, 18.57s/it, lr=1e-6, step_loss=0.0257]\u001b[0m\n",
      "\u001b[34mSteps:  10%|█         | 10/100 [03:22<27:50, 18.57s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[35mSteps:  11%|█         | 11/100 [03:28<27:33, 18.58s/it, lr=1e-6, step_loss=0.0257]\u001b[0m\n",
      "\u001b[35mSteps:  11%|█         | 11/100 [03:28<27:33, 18.58s/it, lr=1e-6, step_loss=0.056]\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:35:04,839] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 11/100 [03:27<27:32, 18.57s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 11/100 [03:27<27:32, 18.57s/it, lr=1e-6, step_loss=0.0237]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 11/100 [03:31<27:32, 18.57s/it, lr=1e-6, step_loss=0.0433]\u001b[0m\n",
      "\u001b[35mSteps:  11%|█         | 11/100 [03:32<27:33, 18.58s/it, lr=1e-6, step_loss=0.0458]\u001b[0m\n",
      "\u001b[35mSteps:  11%|█         | 11/100 [03:36<27:33, 18.58s/it, lr=1e-6, step_loss=0.0135]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 11/100 [03:36<27:32, 18.57s/it, lr=1e-6, step_loss=0.156]\u001b[0m\n",
      "\u001b[35mSteps:  11%|█         | 11/100 [03:41<27:33, 18.58s/it, lr=1e-6, step_loss=0.0684]\u001b[0m\n",
      "\u001b[34mSteps:  11%|█         | 11/100 [03:40<27:32, 18.57s/it, lr=1e-6, step_loss=0.0366]\u001b[0m\n",
      "\u001b[35mSteps:  12%|█▏        | 12/100 [03:52<29:40, 20.23s/it, lr=1e-6, step_loss=0.0684]\u001b[0m\n",
      "\u001b[35mSteps:  12%|█▏        | 12/100 [03:52<29:40, 20.23s/it, lr=1e-6, step_loss=0.0827]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 12/100 [03:51<29:40, 20.23s/it, lr=1e-6, step_loss=0.0366]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 12/100 [03:51<29:40, 20.23s/it, lr=1e-6, step_loss=0.00741]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 12/100 [03:55<29:40, 20.23s/it, lr=1e-6, step_loss=0.0946]\u001b[0m\n",
      "\u001b[35mSteps:  12%|█▏        | 12/100 [03:56<29:40, 20.23s/it, lr=1e-6, step_loss=0.0203]\u001b[0m\n",
      "\u001b[35mSteps:  12%|█▏        | 12/100 [04:00<29:40, 20.23s/it, lr=1e-6, step_loss=0.0615]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 12/100 [04:00<29:40, 20.23s/it, lr=1e-6, step_loss=0.00577]\u001b[0m\n",
      "\u001b[35mSteps:  12%|█▏        | 12/100 [04:05<29:40, 20.23s/it, lr=1e-6, step_loss=0.026]\u001b[0m\n",
      "\u001b[34mSteps:  12%|█▏        | 12/100 [04:04<29:40, 20.23s/it, lr=1e-6, step_loss=0.0179]\u001b[0m\n",
      "\u001b[35mSteps:  13%|█▎        | 13/100 [04:15<30:32, 21.07s/it, lr=1e-6, step_loss=0.026]\u001b[0m\n",
      "\u001b[35mSteps:  13%|█▎        | 13/100 [04:15<30:32, 21.07s/it, lr=1e-6, step_loss=0.0499]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 13/100 [04:14<30:32, 21.06s/it, lr=1e-6, step_loss=0.0179]\u001b[0m\n",
      "\u001b[34mSteps:  13%|█▎        | 13/100 [04:14<30:32, 21.06s/it, lr=1e-6, step_loss=0.0995]\u001b[0m\n",
      "\u001b[35mSteps:  14%|█▍        | 14/100 [04:19<23:05, 16.11s/it, lr=1e-6, step_loss=0.0499]\u001b[0m\n",
      "\u001b[35mSteps:  14%|█▍        | 14/100 [04:19<23:05, 16.11s/it, lr=1e-6, step_loss=0.0679]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 14/100 [04:19<23:05, 16.12s/it, lr=1e-6, step_loss=0.0995]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 14/100 [04:19<23:05, 16.12s/it, lr=1e-6, step_loss=0.0152]\u001b[0m\n",
      "\u001b[34m03/22/2024 16:35:56 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt:  .\u001b[0m\n",
      "\u001b[34mFetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mFetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 18.72it/s]#033[A\u001b[0m\n",
      "\u001b[34mFetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 18.71it/s]\u001b[0m\n",
      "\u001b[34mLoading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00, 18.02it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoading pipeline components...:  71%|███████▏  | 5/7 [00:00<00:00, 12.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.38it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.47it/s]\u001b[0m\n",
      "\u001b[35mSteps:  14%|█▍        | 14/100 [05:08<23:05, 16.11s/it, lr=1e-6, step_loss=0.0816]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 14/100 [05:07<23:05, 16.12s/it, lr=1e-6, step_loss=0.0231]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 14/100 [05:12<23:05, 16.12s/it, lr=1e-6, step_loss=0.0801]\u001b[0m\n",
      "\u001b[35mSteps:  14%|█▍        | 14/100 [05:13<23:05, 16.11s/it, lr=1e-6, step_loss=0.0447]\u001b[0m\n",
      "\u001b[35mSteps:  14%|█▍        | 14/100 [05:25<23:05, 16.11s/it, lr=1e-6, step_loss=0.0652]\u001b[0m\n",
      "\u001b[34mSteps:  14%|█▍        | 14/100 [05:25<23:05, 16.12s/it, lr=1e-6, step_loss=0.0146]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 15/100 [05:29<46:01, 32.49s/it, lr=1e-6, step_loss=0.0146]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 15/100 [05:29<46:01, 32.49s/it, lr=1e-6, step_loss=0.0597]\u001b[0m\n",
      "\u001b[35mSteps:  15%|█▌        | 15/100 [05:30<46:01, 32.48s/it, lr=1e-6, step_loss=0.0652]\u001b[0m\n",
      "\u001b[35mSteps:  15%|█▌        | 15/100 [05:30<46:01, 32.48s/it, lr=1e-6, step_loss=0.052]\u001b[0m\n",
      "\u001b[35mSteps:  15%|█▌        | 15/100 [05:34<46:01, 32.48s/it, lr=1e-6, step_loss=0.0142]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 15/100 [05:34<46:01, 32.49s/it, lr=1e-6, step_loss=0.00668]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 15/100 [05:38<46:01, 32.49s/it, lr=1e-6, step_loss=0.0361]\u001b[0m\n",
      "\u001b[35mSteps:  15%|█▌        | 15/100 [05:39<46:01, 32.48s/it, lr=1e-6, step_loss=0.0602]\u001b[0m\n",
      "\u001b[35mSteps:  15%|█▌        | 15/100 [05:48<46:01, 32.48s/it, lr=1e-6, step_loss=0.0744]\u001b[0m\n",
      "\u001b[34mSteps:  15%|█▌        | 15/100 [05:48<46:01, 32.49s/it, lr=1e-6, step_loss=0.0836]\u001b[0m\n",
      "\u001b[35mSteps:  16%|█▌        | 16/100 [05:53<41:28, 29.62s/it, lr=1e-6, step_loss=0.0744]\u001b[0m\n",
      "\u001b[35mSteps:  16%|█▌        | 16/100 [05:53<41:28, 29.62s/it, lr=1e-6, step_loss=0.00318]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 16/100 [05:52<41:28, 29.63s/it, lr=1e-6, step_loss=0.0836]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 16/100 [05:52<41:28, 29.63s/it, lr=1e-6, step_loss=0.0339]\u001b[0m\n",
      "\u001b[35mSteps:  16%|█▌        | 16/100 [05:57<41:28, 29.62s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 16/100 [05:57<41:28, 29.63s/it, lr=1e-6, step_loss=0.0531]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 16/100 [06:01<41:28, 29.63s/it, lr=1e-6, step_loss=0.0356]\u001b[0m\n",
      "\u001b[35mSteps:  16%|█▌        | 16/100 [06:02<41:28, 29.62s/it, lr=1e-6, step_loss=0.0103]\u001b[0m\n",
      "\u001b[35mSteps:  16%|█▌        | 16/100 [06:11<41:28, 29.62s/it, lr=1e-6, step_loss=0.0403]\u001b[0m\n",
      "\u001b[34mSteps:  16%|█▌        | 16/100 [06:11<41:28, 29.63s/it, lr=1e-6, step_loss=0.0124]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 17/100 [06:15<38:14, 27.65s/it, lr=1e-6, step_loss=0.0124]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 17/100 [06:15<38:14, 27.65s/it, lr=1e-6, step_loss=0.076]\u001b[0m\n",
      "\u001b[35mSteps:  17%|█▋        | 17/100 [06:16<38:14, 27.65s/it, lr=1e-6, step_loss=0.0403]\u001b[0m\n",
      "\u001b[35mSteps:  17%|█▋        | 17/100 [06:16<38:14, 27.65s/it, lr=1e-6, step_loss=0.0326]\u001b[0m\n",
      "\u001b[35mSteps:  17%|█▋        | 17/100 [06:20<38:14, 27.65s/it, lr=1e-6, step_loss=0.0679]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 17/100 [06:20<38:14, 27.65s/it, lr=1e-6, step_loss=0.0289]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 17/100 [06:24<38:14, 27.65s/it, lr=1e-6, step_loss=0.105]\u001b[0m\n",
      "\u001b[35mSteps:  17%|█▋        | 17/100 [06:25<38:14, 27.65s/it, lr=1e-6, step_loss=0.0538]\u001b[0m\n",
      "\u001b[35mSteps:  17%|█▋        | 17/100 [06:34<38:14, 27.65s/it, lr=1e-6, step_loss=0.0335]\u001b[0m\n",
      "\u001b[34mSteps:  17%|█▋        | 17/100 [06:34<38:14, 27.65s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 18/100 [06:38<35:52, 26.26s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 18/100 [06:38<35:52, 26.26s/it, lr=1e-6, step_loss=0.107]\u001b[0m\n",
      "\u001b[35mSteps:  18%|█▊        | 18/100 [06:39<35:52, 26.25s/it, lr=1e-6, step_loss=0.0335]\u001b[0m\n",
      "\u001b[35mSteps:  18%|█▊        | 18/100 [06:39<35:52, 26.25s/it, lr=1e-6, step_loss=0.0641]\u001b[0m\n",
      "\u001b[35mSteps:  18%|█▊        | 18/100 [06:43<35:52, 26.25s/it, lr=1e-6, step_loss=0.0416]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 18/100 [06:43<35:52, 26.26s/it, lr=1e-6, step_loss=0.0189]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 18/100 [06:47<35:52, 26.26s/it, lr=1e-6, step_loss=0.0397]\u001b[0m\n",
      "\u001b[35mSteps:  18%|█▊        | 18/100 [06:48<35:52, 26.25s/it, lr=1e-6, step_loss=0.0564]\u001b[0m\n",
      "\u001b[35mSteps:  18%|█▊        | 18/100 [06:57<35:52, 26.25s/it, lr=1e-6, step_loss=0.137]\u001b[0m\n",
      "\u001b[34mSteps:  18%|█▊        | 18/100 [06:57<35:52, 26.26s/it, lr=1e-6, step_loss=0.0471]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 19/100 [07:01<34:09, 25.30s/it, lr=1e-6, step_loss=0.0471]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 19/100 [07:01<34:09, 25.30s/it, lr=1e-6, step_loss=0.00901]\u001b[0m\n",
      "\u001b[35mSteps:  19%|█▉        | 19/100 [07:02<34:09, 25.30s/it, lr=1e-6, step_loss=0.137]\u001b[0m\n",
      "\u001b[35mSteps:  19%|█▉        | 19/100 [07:02<34:09, 25.30s/it, lr=1e-6, step_loss=0.015]\u001b[0m\n",
      "\u001b[35mSteps:  19%|█▉        | 19/100 [07:06<34:09, 25.30s/it, lr=1e-6, step_loss=0.0799]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 19/100 [07:06<34:09, 25.30s/it, lr=1e-6, step_loss=0.0437]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 19/100 [07:10<34:09, 25.30s/it, lr=1e-6, step_loss=0.0749]\u001b[0m\n",
      "\u001b[35mSteps:  19%|█▉        | 19/100 [07:11<34:09, 25.30s/it, lr=1e-6, step_loss=0.0747]\u001b[0m\n",
      "\u001b[35mSteps:  19%|█▉        | 19/100 [07:20<34:09, 25.30s/it, lr=1e-6, step_loss=0.0096]\u001b[0m\n",
      "\u001b[34mSteps:  19%|█▉        | 19/100 [07:20<34:09, 25.30s/it, lr=1e-6, step_loss=0.135]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 20/100 [07:24<32:49, 24.62s/it, lr=1e-6, step_loss=0.135]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 20/100 [07:24<32:49, 24.62s/it, lr=1e-6, step_loss=0.00934]\u001b[0m\n",
      "\u001b[35mSteps:  20%|██        | 20/100 [07:25<32:49, 24.61s/it, lr=1e-6, step_loss=0.0096]\u001b[0m\n",
      "\u001b[35mSteps:  20%|██        | 20/100 [07:25<32:49, 24.61s/it, lr=1e-6, step_loss=0.0486]\u001b[0m\n",
      "\u001b[35mSteps:  20%|██        | 20/100 [07:29<32:49, 24.61s/it, lr=1e-6, step_loss=0.0284]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 20/100 [07:29<32:49, 24.62s/it, lr=1e-6, step_loss=0.00515]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 20/100 [07:33<32:49, 24.62s/it, lr=1e-6, step_loss=0.0483]\u001b[0m\n",
      "\u001b[35mSteps:  20%|██        | 20/100 [07:34<32:49, 24.61s/it, lr=1e-6, step_loss=0.0635]\u001b[0m\n",
      "\u001b[35mSteps:  20%|██        | 20/100 [07:43<32:49, 24.61s/it, lr=1e-6, step_loss=0.0933]\u001b[0m\n",
      "\u001b[34mSteps:  20%|██        | 20/100 [07:43<32:49, 24.62s/it, lr=1e-6, step_loss=0.064]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 21/100 [07:47<31:46, 24.14s/it, lr=1e-6, step_loss=0.064]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 21/100 [07:47<31:46, 24.14s/it, lr=1e-6, step_loss=0.0049]\u001b[0m\n",
      "\u001b[35mSteps:  21%|██        | 21/100 [07:48<31:46, 24.14s/it, lr=1e-6, step_loss=0.0933]\u001b[0m\n",
      "\u001b[35mSteps:  21%|██        | 21/100 [07:48<31:46, 24.14s/it, lr=1e-6, step_loss=0.108]\u001b[0m\n",
      "\u001b[35mSteps:  21%|██        | 21/100 [07:52<31:46, 24.14s/it, lr=1e-6, step_loss=0.0596]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 21/100 [07:52<31:46, 24.14s/it, lr=1e-6, step_loss=0.0252]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 21/100 [07:56<31:46, 24.14s/it, lr=1e-6, step_loss=0.0305]\u001b[0m\n",
      "\u001b[35mSteps:  21%|██        | 21/100 [07:57<31:46, 24.14s/it, lr=1e-6, step_loss=0.0115]\u001b[0m\n",
      "\u001b[35mSteps:  21%|██        | 21/100 [08:06<31:46, 24.14s/it, lr=1e-6, step_loss=0.111]\u001b[0m\n",
      "\u001b[34mSteps:  21%|██        | 21/100 [08:06<31:46, 24.14s/it, lr=1e-6, step_loss=0.0534]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 22/100 [08:10<30:55, 23.79s/it, lr=1e-6, step_loss=0.0534]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 22/100 [08:10<30:55, 23.79s/it, lr=1e-6, step_loss=0.0839]\u001b[0m\n",
      "\u001b[35mSteps:  22%|██▏       | 22/100 [08:11<30:55, 23.79s/it, lr=1e-6, step_loss=0.111]\u001b[0m\n",
      "\u001b[35mSteps:  22%|██▏       | 22/100 [08:11<30:55, 23.79s/it, lr=1e-6, step_loss=0.0238]\u001b[0m\n",
      "\u001b[35mSteps:  22%|██▏       | 22/100 [08:15<30:55, 23.79s/it, lr=1e-6, step_loss=0.0592]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 22/100 [08:15<30:55, 23.79s/it, lr=1e-6, step_loss=0.0957]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 22/100 [08:19<30:55, 23.79s/it, lr=1e-6, step_loss=0.133]\u001b[0m\n",
      "\u001b[35mSteps:  22%|██▏       | 22/100 [08:20<30:55, 23.79s/it, lr=1e-6, step_loss=0.0459]\u001b[0m\n",
      "\u001b[35mSteps:  22%|██▏       | 22/100 [08:29<30:55, 23.79s/it, lr=1e-6, step_loss=0.00431]\u001b[0m\n",
      "\u001b[34mSteps:  22%|██▏       | 22/100 [08:29<30:55, 23.79s/it, lr=1e-6, step_loss=0.0488]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 23/100 [08:33<30:12, 23.54s/it, lr=1e-6, step_loss=0.0488]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 23/100 [08:33<30:12, 23.54s/it, lr=1e-6, step_loss=0.0382]\u001b[0m\n",
      "\u001b[35mSteps:  23%|██▎       | 23/100 [08:34<30:12, 23.54s/it, lr=1e-6, step_loss=0.00431]\u001b[0m\n",
      "\u001b[35mSteps:  23%|██▎       | 23/100 [08:34<30:12, 23.54s/it, lr=1e-6, step_loss=0.0415]\u001b[0m\n",
      "\u001b[35mSteps:  23%|██▎       | 23/100 [08:38<30:12, 23.54s/it, lr=1e-6, step_loss=0.0185]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 23/100 [08:38<30:12, 23.54s/it, lr=1e-6, step_loss=0.0305]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 23/100 [08:42<30:12, 23.54s/it, lr=1e-6, step_loss=0.0339]\u001b[0m\n",
      "\u001b[35mSteps:  23%|██▎       | 23/100 [08:43<30:12, 23.54s/it, lr=1e-6, step_loss=0.0509]\u001b[0m\n",
      "\u001b[35mSteps:  23%|██▎       | 23/100 [08:52<30:12, 23.54s/it, lr=1e-6, step_loss=0.0537]\u001b[0m\n",
      "\u001b[34mSteps:  23%|██▎       | 23/100 [08:52<30:12, 23.54s/it, lr=1e-6, step_loss=0.0376]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 24/100 [08:56<29:39, 23.41s/it, lr=1e-6, step_loss=0.0376]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 24/100 [08:56<29:39, 23.41s/it, lr=1e-6, step_loss=0.0158]\u001b[0m\n",
      "\u001b[35mSteps:  24%|██▍       | 24/100 [08:57<29:39, 23.41s/it, lr=1e-6, step_loss=0.0537]\u001b[0m\n",
      "\u001b[35mSteps:  24%|██▍       | 24/100 [08:57<29:39, 23.41s/it, lr=1e-6, step_loss=0.0807]\u001b[0m\n",
      "\u001b[35mSteps:  24%|██▍       | 24/100 [09:01<29:39, 23.41s/it, lr=1e-6, step_loss=0.098]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 24/100 [09:01<29:39, 23.41s/it, lr=1e-6, step_loss=0.0475]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 24/100 [09:05<29:39, 23.41s/it, lr=1e-6, step_loss=0.014]\u001b[0m\n",
      "\u001b[35mSteps:  24%|██▍       | 24/100 [09:06<29:39, 23.41s/it, lr=1e-6, step_loss=0.0489]\u001b[0m\n",
      "\u001b[35mSteps:  24%|██▍       | 24/100 [09:15<29:39, 23.41s/it, lr=1e-6, step_loss=0.0123]\u001b[0m\n",
      "\u001b[34mSteps:  24%|██▍       | 24/100 [09:15<29:39, 23.41s/it, lr=1e-6, step_loss=0.0374]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 25/100 [09:19<29:04, 23.26s/it, lr=1e-6, step_loss=0.0374]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 25/100 [09:19<29:04, 23.26s/it, lr=1e-6, step_loss=0.101]\u001b[0m\n",
      "\u001b[35mSteps:  25%|██▌       | 25/100 [09:20<29:04, 23.26s/it, lr=1e-6, step_loss=0.0123]\u001b[0m\n",
      "\u001b[35mSteps:  25%|██▌       | 25/100 [09:20<29:04, 23.26s/it, lr=1e-6, step_loss=0.0527]\u001b[0m\n",
      "\u001b[35mSteps:  25%|██▌       | 25/100 [09:24<29:04, 23.26s/it, lr=1e-6, step_loss=0.0317]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 25/100 [09:24<29:04, 23.26s/it, lr=1e-6, step_loss=0.0178]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 25/100 [09:28<29:04, 23.26s/it, lr=1e-6, step_loss=0.102]\u001b[0m\n",
      "\u001b[35mSteps:  25%|██▌       | 25/100 [09:29<29:04, 23.26s/it, lr=1e-6, step_loss=0.0697]\u001b[0m\n",
      "\u001b[35mSteps:  25%|██▌       | 25/100 [09:38<29:04, 23.26s/it, lr=1e-6, step_loss=0.0306]\u001b[0m\n",
      "\u001b[34mSteps:  25%|██▌       | 25/100 [09:38<29:04, 23.26s/it, lr=1e-6, step_loss=0.0507]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 26/100 [09:42<28:34, 23.17s/it, lr=1e-6, step_loss=0.0507]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 26/100 [09:42<28:34, 23.17s/it, lr=1e-6, step_loss=0.0128]\u001b[0m\n",
      "\u001b[35mSteps:  26%|██▌       | 26/100 [09:43<28:34, 23.17s/it, lr=1e-6, step_loss=0.0306]\u001b[0m\n",
      "\u001b[35mSteps:  26%|██▌       | 26/100 [09:43<28:34, 23.17s/it, lr=1e-6, step_loss=0.0459]\u001b[0m\n",
      "\u001b[35mSteps:  26%|██▌       | 26/100 [09:47<28:34, 23.17s/it, lr=1e-6, step_loss=0.0292]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 26/100 [09:47<28:34, 23.17s/it, lr=1e-6, step_loss=0.0406]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 26/100 [09:51<28:34, 23.17s/it, lr=1e-6, step_loss=0.0248]\u001b[0m\n",
      "\u001b[35mSteps:  26%|██▌       | 26/100 [09:52<28:34, 23.17s/it, lr=1e-6, step_loss=0.0719]\u001b[0m\n",
      "\u001b[35mSteps:  26%|██▌       | 26/100 [10:01<28:34, 23.17s/it, lr=1e-6, step_loss=0.106]\u001b[0m\n",
      "\u001b[34mSteps:  26%|██▌       | 26/100 [10:01<28:34, 23.17s/it, lr=1e-6, step_loss=0.0318]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 27/100 [10:05<28:09, 23.14s/it, lr=1e-6, step_loss=0.0318]\u001b[0m\n",
      "\u001b[34mSteps:  27%|██▋       | 27/100 [10:05<28:09, 23.14s/it, lr=1e-6, step_loss=0.0295]\u001b[0m\n",
      "\u001b[35mSteps:  27%|██▋       | 27/100 [10:06<28:09, 23.14s/it, lr=1e-6, step_loss=0.106]\u001b[0m\n",
      "\u001b[35mSteps:  27%|██▋       | 27/100 [10:06<28:09, 23.14s/it, lr=1e-6, step_loss=0.12]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 28/100 [10:10<21:16, 17.73s/it, lr=1e-6, step_loss=0.0295]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 28/100 [10:10<21:16, 17.73s/it, lr=1e-6, step_loss=0.0868]\u001b[0m\n",
      "\u001b[35mSteps:  28%|██▊       | 28/100 [10:11<21:16, 17.72s/it, lr=1e-6, step_loss=0.12]\u001b[0m\n",
      "\u001b[35mSteps:  28%|██▊       | 28/100 [10:11<21:16, 17.72s/it, lr=1e-6, step_loss=0.0214]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 28/100 [10:16<21:16, 17.73s/it, lr=1e-6, step_loss=0.0259]\u001b[0m\n",
      "\u001b[35mSteps:  28%|██▊       | 28/100 [10:17<21:16, 17.72s/it, lr=1e-6, step_loss=0.0568]\u001b[0m\n",
      "\u001b[35mSteps:  28%|██▊       | 28/100 [10:29<21:16, 17.72s/it, lr=1e-6, step_loss=0.00474]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 28/100 [10:29<21:16, 17.73s/it, lr=1e-6, step_loss=0.0369]\u001b[0m\n",
      "\u001b[34mSteps:  28%|██▊       | 28/100 [10:33<21:16, 17.73s/it, lr=1e-6, step_loss=0.0692]\u001b[0m\n",
      "\u001b[35mSteps:  28%|██▊       | 28/100 [10:34<21:16, 17.72s/it, lr=1e-6, step_loss=0.0476]\u001b[0m\n",
      "\u001b[35mSteps:  29%|██▉       | 29/100 [10:38<24:26, 20.65s/it, lr=1e-6, step_loss=0.0476]\u001b[0m\n",
      "\u001b[35mSteps:  29%|██▉       | 29/100 [10:38<24:26, 20.65s/it, lr=1e-6, step_loss=0.0564]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 29/100 [10:38<24:26, 20.66s/it, lr=1e-6, step_loss=0.0692]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 29/100 [10:38<24:26, 20.66s/it, lr=1e-6, step_loss=0.0362]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 29/100 [10:42<24:26, 20.66s/it, lr=1e-6, step_loss=0.12]\u001b[0m\n",
      "\u001b[35mSteps:  29%|██▉       | 29/100 [10:43<24:26, 20.65s/it, lr=1e-6, step_loss=0.0398]\u001b[0m\n",
      "\u001b[35mSteps:  29%|██▉       | 29/100 [10:52<24:26, 20.65s/it, lr=1e-6, step_loss=0.0366]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 29/100 [10:52<24:26, 20.66s/it, lr=1e-6, step_loss=0.0327]\u001b[0m\n",
      "\u001b[34mSteps:  29%|██▉       | 29/100 [10:56<24:26, 20.66s/it, lr=1e-6, step_loss=0.0632]\u001b[0m\n",
      "\u001b[35mSteps:  29%|██▉       | 29/100 [10:57<24:26, 20.65s/it, lr=1e-6, step_loss=0.0628]\u001b[0m\n",
      "\u001b[35mSteps:  30%|███       | 30/100 [11:01<24:56, 21.38s/it, lr=1e-6, step_loss=0.0628]\u001b[0m\n",
      "\u001b[35mSteps:  30%|███       | 30/100 [11:01<24:56, 21.38s/it, lr=1e-6, step_loss=0.00957]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 30/100 [11:01<24:56, 21.38s/it, lr=1e-6, step_loss=0.0632]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 30/100 [11:01<24:56, 21.38s/it, lr=1e-6, step_loss=0.00408]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 30/100 [11:05<24:56, 21.38s/it, lr=1e-6, step_loss=0.0358]\u001b[0m\n",
      "\u001b[35mSteps:  30%|███       | 30/100 [11:06<24:56, 21.38s/it, lr=1e-6, step_loss=0.0108]\u001b[0m\n",
      "\u001b[35mSteps:  30%|███       | 30/100 [11:16<24:56, 21.38s/it, lr=1e-6, step_loss=0.00241]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 30/100 [11:15<24:56, 21.38s/it, lr=1e-6, step_loss=0.0349]\u001b[0m\n",
      "\u001b[34mSteps:  30%|███       | 30/100 [11:19<24:56, 21.38s/it, lr=1e-6, step_loss=0.0339]\u001b[0m\n",
      "\u001b[35mSteps:  30%|███       | 30/100 [11:20<24:56, 21.38s/it, lr=1e-6, step_loss=0.0364]\u001b[0m\n",
      "\u001b[35mSteps:  31%|███       | 31/100 [11:25<25:10, 21.89s/it, lr=1e-6, step_loss=0.0364]\u001b[0m\n",
      "\u001b[35mSteps:  31%|███       | 31/100 [11:25<25:10, 21.89s/it, lr=1e-6, step_loss=0.0481]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 31/100 [11:24<25:10, 21.90s/it, lr=1e-6, step_loss=0.0339]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 31/100 [11:24<25:10, 21.90s/it, lr=1e-6, step_loss=0.00992]\u001b[0m\n",
      "\u001b[35mSteps:  31%|███       | 31/100 [11:29<25:10, 21.89s/it, lr=1e-6, step_loss=0.0728]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 31/100 [11:29<25:10, 21.90s/it, lr=1e-6, step_loss=0.00656]\u001b[0m\n",
      "\u001b[35mSteps:  31%|███       | 31/100 [11:39<25:10, 21.89s/it, lr=1e-6, step_loss=0.0842]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 31/100 [11:38<25:10, 21.90s/it, lr=1e-6, step_loss=0.0336]\u001b[0m\n",
      "\u001b[34mSteps:  31%|███       | 31/100 [11:42<25:10, 21.90s/it, lr=1e-6, step_loss=0.0281]\u001b[0m\n",
      "\u001b[35mSteps:  31%|███       | 31/100 [11:43<25:10, 21.89s/it, lr=1e-6, step_loss=0.0585]\u001b[0m\n",
      "\u001b[35mSteps:  32%|███▏      | 32/100 [11:48<25:12, 22.24s/it, lr=1e-6, step_loss=0.0585]\u001b[0m\n",
      "\u001b[35mSteps:  32%|███▏      | 32/100 [11:48<25:12, 22.24s/it, lr=1e-6, step_loss=0.0213]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 32/100 [11:47<25:12, 22.24s/it, lr=1e-6, step_loss=0.0281]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 32/100 [11:47<25:12, 22.24s/it, lr=1e-6, step_loss=0.0833]\u001b[0m\n",
      "\u001b[35mSteps:  32%|███▏      | 32/100 [11:52<25:12, 22.24s/it, lr=1e-6, step_loss=0.0366]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 32/100 [11:52<25:12, 22.24s/it, lr=1e-6, step_loss=0.0112]\u001b[0m\n",
      "\u001b[35mSteps:  32%|███▏      | 32/100 [12:02<25:12, 22.24s/it, lr=1e-6, step_loss=0.0757]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 32/100 [12:01<25:12, 22.24s/it, lr=1e-6, step_loss=0.0555]\u001b[0m\n",
      "\u001b[34mSteps:  32%|███▏      | 32/100 [12:05<25:12, 22.24s/it, lr=1e-6, step_loss=0.107]\u001b[0m\n",
      "\u001b[35mSteps:  32%|███▏      | 32/100 [12:06<25:12, 22.24s/it, lr=1e-6, step_loss=0.0502]\u001b[0m\n",
      "\u001b[35mSteps:  33%|███▎      | 33/100 [12:11<25:06, 22.48s/it, lr=1e-6, step_loss=0.0502]\u001b[0m\n",
      "\u001b[35mSteps:  33%|███▎      | 33/100 [12:11<25:06, 22.48s/it, lr=1e-6, step_loss=0.0848]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 33/100 [12:10<25:06, 22.49s/it, lr=1e-6, step_loss=0.107]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 33/100 [12:10<25:06, 22.49s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[35mSteps:  33%|███▎      | 33/100 [12:15<25:06, 22.48s/it, lr=1e-6, step_loss=0.0386]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 33/100 [12:15<25:06, 22.49s/it, lr=1e-6, step_loss=0.0482]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 33/100 [12:24<25:06, 22.49s/it, lr=1e-6, step_loss=0.0213]\u001b[0m\n",
      "\u001b[35mSteps:  33%|███▎      | 33/100 [12:25<25:06, 22.48s/it, lr=1e-6, step_loss=0.00583]\u001b[0m\n",
      "\u001b[35mSteps:  33%|███▎      | 33/100 [12:29<25:06, 22.48s/it, lr=1e-6, step_loss=0.0688]\u001b[0m\n",
      "\u001b[34mSteps:  33%|███▎      | 33/100 [12:28<25:06, 22.49s/it, lr=1e-6, step_loss=0.0952]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 34/100 [12:33<24:54, 22.65s/it, lr=1e-6, step_loss=0.0952]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 34/100 [12:33<24:54, 22.65s/it, lr=1e-6, step_loss=0.0624]\u001b[0m\n",
      "\u001b[35mSteps:  34%|███▍      | 34/100 [12:34<24:54, 22.65s/it, lr=1e-6, step_loss=0.0688]\u001b[0m\n",
      "\u001b[35mSteps:  34%|███▍      | 34/100 [12:34<24:54, 22.65s/it, lr=1e-6, step_loss=0.0339]\u001b[0m\n",
      "\u001b[35mSteps:  34%|███▍      | 34/100 [12:38<24:54, 22.65s/it, lr=1e-6, step_loss=0.119]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 34/100 [12:38<24:54, 22.65s/it, lr=1e-6, step_loss=0.0623]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 34/100 [12:47<24:54, 22.65s/it, lr=1e-6, step_loss=0.0393]\u001b[0m\n",
      "\u001b[35mSteps:  34%|███▍      | 34/100 [12:48<24:54, 22.65s/it, lr=1e-6, step_loss=0.00466]\u001b[0m\n",
      "\u001b[35mSteps:  34%|███▍      | 34/100 [12:52<24:54, 22.65s/it, lr=1e-6, step_loss=0.0668]\u001b[0m\n",
      "\u001b[34mSteps:  34%|███▍      | 34/100 [12:52<24:54, 22.65s/it, lr=1e-6, step_loss=0.0412]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 35/100 [12:56<24:40, 22.78s/it, lr=1e-6, step_loss=0.0412]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 35/100 [12:56<24:40, 22.78s/it, lr=1e-6, step_loss=0.0571]\u001b[0m\n",
      "\u001b[35mSteps:  35%|███▌      | 35/100 [12:57<24:40, 22.77s/it, lr=1e-6, step_loss=0.0668]\u001b[0m\n",
      "\u001b[35mSteps:  35%|███▌      | 35/100 [12:57<24:40, 22.77s/it, lr=1e-6, step_loss=0.0528]\u001b[0m\n",
      "\u001b[35mSteps:  35%|███▌      | 35/100 [13:01<24:40, 22.77s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 35/100 [13:01<24:40, 22.78s/it, lr=1e-6, step_loss=0.0428]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 35/100 [13:10<24:40, 22.78s/it, lr=1e-6, step_loss=0.0935]\u001b[0m\n",
      "\u001b[35mSteps:  35%|███▌      | 35/100 [13:11<24:40, 22.77s/it, lr=1e-6, step_loss=0.037]\u001b[0m\n",
      "\u001b[35mSteps:  35%|███▌      | 35/100 [13:15<24:40, 22.77s/it, lr=1e-6, step_loss=0.0537]\u001b[0m\n",
      "\u001b[34mSteps:  35%|███▌      | 35/100 [13:15<24:40, 22.78s/it, lr=1e-6, step_loss=0.0445]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 36/100 [13:19<24:22, 22.85s/it, lr=1e-6, step_loss=0.0445]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 36/100 [13:19<24:22, 22.85s/it, lr=1e-6, step_loss=0.109]\u001b[0m\n",
      "\u001b[35mSteps:  36%|███▌      | 36/100 [13:20<24:22, 22.85s/it, lr=1e-6, step_loss=0.0537]\u001b[0m\n",
      "\u001b[35mSteps:  36%|███▌      | 36/100 [13:20<24:22, 22.85s/it, lr=1e-6, step_loss=0.0537]\u001b[0m\n",
      "\u001b[35mSteps:  36%|███▌      | 36/100 [13:24<24:22, 22.85s/it, lr=1e-6, step_loss=0.0576]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 36/100 [13:24<24:22, 22.85s/it, lr=1e-6, step_loss=0.0709]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 36/100 [13:33<24:22, 22.85s/it, lr=1e-6, step_loss=0.00983]\u001b[0m\n",
      "\u001b[35mSteps:  36%|███▌      | 36/100 [13:34<24:22, 22.85s/it, lr=1e-6, step_loss=0.0399]\u001b[0m\n",
      "\u001b[35mSteps:  36%|███▌      | 36/100 [13:38<24:22, 22.85s/it, lr=1e-6, step_loss=0.0309]\u001b[0m\n",
      "\u001b[34mSteps:  36%|███▌      | 36/100 [13:38<24:22, 22.85s/it, lr=1e-6, step_loss=0.0421]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 37/100 [13:42<24:02, 22.89s/it, lr=1e-6, step_loss=0.0421]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 37/100 [13:42<24:02, 22.89s/it, lr=1e-6, step_loss=0.0625]\u001b[0m\n",
      "\u001b[35mSteps:  37%|███▋      | 37/100 [13:43<24:02, 22.89s/it, lr=1e-6, step_loss=0.0309]\u001b[0m\n",
      "\u001b[35mSteps:  37%|███▋      | 37/100 [13:43<24:02, 22.89s/it, lr=1e-6, step_loss=0.0117]\u001b[0m\n",
      "\u001b[35mSteps:  37%|███▋      | 37/100 [13:47<24:02, 22.89s/it, lr=1e-6, step_loss=0.00864]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 37/100 [13:47<24:02, 22.89s/it, lr=1e-6, step_loss=0.0859]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 37/100 [13:56<24:02, 22.89s/it, lr=1e-6, step_loss=0.0514]\u001b[0m\n",
      "\u001b[35mSteps:  37%|███▋      | 37/100 [13:57<24:02, 22.89s/it, lr=1e-6, step_loss=0.0638]\u001b[0m\n",
      "\u001b[35mSteps:  37%|███▋      | 37/100 [14:01<24:02, 22.89s/it, lr=1e-6, step_loss=0.0725]\u001b[0m\n",
      "\u001b[34mSteps:  37%|███▋      | 37/100 [14:01<24:02, 22.89s/it, lr=1e-6, step_loss=0.0309]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 38/100 [14:05<23:42, 22.95s/it, lr=1e-6, step_loss=0.0309]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 38/100 [14:05<23:42, 22.95s/it, lr=1e-6, step_loss=0.0178]\u001b[0m\n",
      "\u001b[35mSteps:  38%|███▊      | 38/100 [14:06<23:42, 22.95s/it, lr=1e-6, step_loss=0.0725]\u001b[0m\n",
      "\u001b[35mSteps:  38%|███▊      | 38/100 [14:06<23:42, 22.95s/it, lr=1e-6, step_loss=0.0149]\u001b[0m\n",
      "\u001b[35mSteps:  38%|███▊      | 38/100 [14:10<23:42, 22.95s/it, lr=1e-6, step_loss=0.0881]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 38/100 [14:10<23:42, 22.95s/it, lr=1e-6, step_loss=0.0241]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 38/100 [14:19<23:42, 22.95s/it, lr=1e-6, step_loss=0.0827]\u001b[0m\n",
      "\u001b[35mSteps:  38%|███▊      | 38/100 [14:20<23:42, 22.95s/it, lr=1e-6, step_loss=0.0331]\u001b[0m\n",
      "\u001b[35mSteps:  38%|███▊      | 38/100 [14:24<23:42, 22.95s/it, lr=1e-6, step_loss=0.0376]\u001b[0m\n",
      "\u001b[34mSteps:  38%|███▊      | 38/100 [14:24<23:42, 22.95s/it, lr=1e-6, step_loss=0.0621]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 39/100 [14:28<23:21, 22.98s/it, lr=1e-6, step_loss=0.0621]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 39/100 [14:28<23:21, 22.98s/it, lr=1e-6, step_loss=0.0522]\u001b[0m\n",
      "\u001b[35mSteps:  39%|███▉      | 39/100 [14:29<23:21, 22.98s/it, lr=1e-6, step_loss=0.0376]\u001b[0m\n",
      "\u001b[35mSteps:  39%|███▉      | 39/100 [14:29<23:21, 22.98s/it, lr=1e-6, step_loss=0.039]\u001b[0m\n",
      "\u001b[35mSteps:  39%|███▉      | 39/100 [14:33<23:21, 22.98s/it, lr=1e-6, step_loss=0.0338]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 39/100 [14:33<23:21, 22.98s/it, lr=1e-6, step_loss=0.0447]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 39/100 [14:42<23:21, 22.98s/it, lr=1e-6, step_loss=0.0289]\u001b[0m\n",
      "\u001b[35mSteps:  39%|███▉      | 39/100 [14:43<23:21, 22.98s/it, lr=1e-6, step_loss=0.0116]\u001b[0m\n",
      "\u001b[35mSteps:  39%|███▉      | 39/100 [14:47<23:21, 22.98s/it, lr=1e-6, step_loss=0.0612]\u001b[0m\n",
      "\u001b[34mSteps:  39%|███▉      | 39/100 [14:47<23:21, 22.98s/it, lr=1e-6, step_loss=0.0838]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 40/100 [14:51<22:58, 22.98s/it, lr=1e-6, step_loss=0.0838]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 40/100 [14:51<22:58, 22.98s/it, lr=1e-6, step_loss=0.0274]\u001b[0m\n",
      "\u001b[35mSteps:  40%|████      | 40/100 [14:52<22:58, 22.98s/it, lr=1e-6, step_loss=0.0612]\u001b[0m\n",
      "\u001b[35mSteps:  40%|████      | 40/100 [14:52<22:58, 22.98s/it, lr=1e-6, step_loss=0.0579]\u001b[0m\n",
      "\u001b[35mSteps:  40%|████      | 40/100 [14:56<22:58, 22.98s/it, lr=1e-6, step_loss=0.00923]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 40/100 [14:56<22:58, 22.98s/it, lr=1e-6, step_loss=0.112]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 40/100 [15:05<22:58, 22.98s/it, lr=1e-6, step_loss=0.0121]\u001b[0m\n",
      "\u001b[35mSteps:  40%|████      | 40/100 [15:06<22:58, 22.98s/it, lr=1e-6, step_loss=0.049]\u001b[0m\n",
      "\u001b[35mSteps:  40%|████      | 40/100 [15:10<22:58, 22.98s/it, lr=1e-6, step_loss=0.0689]\u001b[0m\n",
      "\u001b[34mSteps:  40%|████      | 40/100 [15:10<22:58, 22.98s/it, lr=1e-6, step_loss=0.0331]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████      | 41/100 [15:14<22:35, 22.97s/it, lr=1e-6, step_loss=0.0331]\u001b[0m\n",
      "\u001b[34mSteps:  41%|████      | 41/100 [15:14<22:35, 22.97s/it, lr=1e-6, step_loss=0.0268]\u001b[0m\n",
      "\u001b[35mSteps:  41%|████      | 41/100 [15:15<22:35, 22.97s/it, lr=1e-6, step_loss=0.0689]\u001b[0m\n",
      "\u001b[35mSteps:  41%|████      | 41/100 [15:15<22:35, 22.97s/it, lr=1e-6, step_loss=0.0612]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 42/100 [15:19<17:02, 17.63s/it, lr=1e-6, step_loss=0.0268]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 42/100 [15:19<17:02, 17.63s/it, lr=1e-6, step_loss=0.0582]\u001b[0m\n",
      "\u001b[35mSteps:  42%|████▏     | 42/100 [15:20<17:02, 17.63s/it, lr=1e-6, step_loss=0.0612]\u001b[0m\n",
      "\u001b[35mSteps:  42%|████▏     | 42/100 [15:20<17:02, 17.63s/it, lr=1e-6, step_loss=0.0244]\u001b[0m\n",
      "\u001b[35mSteps:  42%|████▏     | 42/100 [15:33<17:02, 17.63s/it, lr=1e-6, step_loss=0.0468]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 42/100 [15:33<17:02, 17.63s/it, lr=1e-6, step_loss=0.0849]\u001b[0m\n",
      "\u001b[35mSteps:  42%|████▏     | 42/100 [15:38<17:02, 17.63s/it, lr=1e-6, step_loss=0.0164]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 42/100 [15:38<17:02, 17.63s/it, lr=1e-6, step_loss=0.0433]\u001b[0m\n",
      "\u001b[34mSteps:  42%|████▏     | 42/100 [15:42<17:02, 17.63s/it, lr=1e-6, step_loss=0.0114]\u001b[0m\n",
      "\u001b[35mSteps:  42%|████▏     | 42/100 [15:43<17:02, 17.63s/it, lr=1e-6, step_loss=0.0452]\u001b[0m\n",
      "\u001b[35mSteps:  43%|████▎     | 43/100 [15:48<19:37, 20.65s/it, lr=1e-6, step_loss=0.0452]\u001b[0m\n",
      "\u001b[35mSteps:  43%|████▎     | 43/100 [15:48<19:37, 20.65s/it, lr=1e-6, step_loss=0.0704]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 43/100 [15:47<19:37, 20.65s/it, lr=1e-6, step_loss=0.0114]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 43/100 [15:47<19:37, 20.65s/it, lr=1e-6, step_loss=0.042]\u001b[0m\n",
      "\u001b[35mSteps:  43%|████▎     | 43/100 [15:57<19:37, 20.65s/it, lr=1e-6, step_loss=0.0204]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 43/100 [15:57<19:37, 20.65s/it, lr=1e-6, step_loss=0.0793]\u001b[0m\n",
      "\u001b[35mSteps:  43%|████▎     | 43/100 [16:02<19:37, 20.65s/it, lr=1e-6, step_loss=0.0764]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 43/100 [16:01<19:37, 20.65s/it, lr=1e-6, step_loss=0.00488]\u001b[0m\n",
      "\u001b[34mSteps:  43%|████▎     | 43/100 [16:05<19:37, 20.65s/it, lr=1e-6, step_loss=0.0382]\u001b[0m\n",
      "\u001b[35mSteps:  43%|████▎     | 43/100 [16:06<19:37, 20.65s/it, lr=1e-6, step_loss=0.0114]\u001b[0m\n",
      "\u001b[35mSteps:  44%|████▍     | 44/100 [16:11<19:57, 21.38s/it, lr=1e-6, step_loss=0.0114]\u001b[0m\n",
      "\u001b[35mSteps:  44%|████▍     | 44/100 [16:11<19:57, 21.38s/it, lr=1e-6, step_loss=0.0649]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 44/100 [16:10<19:57, 21.38s/it, lr=1e-6, step_loss=0.0382]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 44/100 [16:10<19:57, 21.38s/it, lr=1e-6, step_loss=0.0909]\u001b[0m\n",
      "\u001b[35mSteps:  44%|████▍     | 44/100 [16:20<19:57, 21.38s/it, lr=1e-6, step_loss=0.0645]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 44/100 [16:20<19:57, 21.38s/it, lr=1e-6, step_loss=0.0693]\u001b[0m\n",
      "\u001b[35mSteps:  44%|████▍     | 44/100 [16:25<19:57, 21.38s/it, lr=1e-6, step_loss=0.0451]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 44/100 [16:24<19:57, 21.38s/it, lr=1e-6, step_loss=0.0727]\u001b[0m\n",
      "\u001b[35mSteps:  44%|████▍     | 44/100 [16:29<19:57, 21.38s/it, lr=1e-6, step_loss=0.021]\u001b[0m\n",
      "\u001b[34mSteps:  44%|████▍     | 44/100 [16:29<19:57, 21.38s/it, lr=1e-6, step_loss=0.0798]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 45/100 [16:33<20:04, 21.89s/it, lr=1e-6, step_loss=0.0798]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 45/100 [16:33<20:04, 21.89s/it, lr=1e-6, step_loss=0.0454]\u001b[0m\n",
      "\u001b[35mSteps:  45%|████▌     | 45/100 [16:34<20:04, 21.89s/it, lr=1e-6, step_loss=0.021]\u001b[0m\n",
      "\u001b[35mSteps:  45%|████▌     | 45/100 [16:34<20:04, 21.89s/it, lr=1e-6, step_loss=0.0423]\u001b[0m\n",
      "\u001b[35mSteps:  45%|████▌     | 45/100 [16:43<20:04, 21.89s/it, lr=1e-6, step_loss=0.101]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 45/100 [16:43<20:04, 21.89s/it, lr=1e-6, step_loss=0.0736]\u001b[0m\n",
      "\u001b[35mSteps:  45%|████▌     | 45/100 [16:48<20:04, 21.89s/it, lr=1e-6, step_loss=0.0242]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 45/100 [16:47<20:04, 21.89s/it, lr=1e-6, step_loss=0.0604]\u001b[0m\n",
      "\u001b[35mSteps:  45%|████▌     | 45/100 [16:52<20:04, 21.89s/it, lr=1e-6, step_loss=0.0219]\u001b[0m\n",
      "\u001b[34mSteps:  45%|████▌     | 45/100 [16:52<20:04, 21.89s/it, lr=1e-6, step_loss=0.00802]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 46/100 [16:56<20:01, 22.25s/it, lr=1e-6, step_loss=0.00802]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 46/100 [16:56<20:01, 22.25s/it, lr=1e-6, step_loss=0.0121]\u001b[0m\n",
      "\u001b[35mSteps:  46%|████▌     | 46/100 [16:57<20:01, 22.25s/it, lr=1e-6, step_loss=0.0219]\u001b[0m\n",
      "\u001b[35mSteps:  46%|████▌     | 46/100 [16:57<20:01, 22.25s/it, lr=1e-6, step_loss=0.0582]\u001b[0m\n",
      "\u001b[35mSteps:  46%|████▌     | 46/100 [17:06<20:01, 22.25s/it, lr=1e-6, step_loss=0.0348]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 46/100 [17:06<20:01, 22.25s/it, lr=1e-6, step_loss=0.0163]\u001b[0m\n",
      "\u001b[35mSteps:  46%|████▌     | 46/100 [17:11<20:01, 22.25s/it, lr=1e-6, step_loss=0.0145]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 46/100 [17:10<20:01, 22.25s/it, lr=1e-6, step_loss=0.0535]\u001b[0m\n",
      "\u001b[35mSteps:  46%|████▌     | 46/100 [17:15<20:01, 22.25s/it, lr=1e-6, step_loss=0.145]\u001b[0m\n",
      "\u001b[34mSteps:  46%|████▌     | 46/100 [17:15<20:01, 22.25s/it, lr=1e-6, step_loss=0.0144]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 47/100 [17:19<19:50, 22.46s/it, lr=1e-6, step_loss=0.0144]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 47/100 [17:19<19:50, 22.46s/it, lr=1e-6, step_loss=0.0432]\u001b[0m\n",
      "\u001b[35mSteps:  47%|████▋     | 47/100 [17:20<19:50, 22.46s/it, lr=1e-6, step_loss=0.145]\u001b[0m\n",
      "\u001b[35mSteps:  47%|████▋     | 47/100 [17:20<19:50, 22.46s/it, lr=1e-6, step_loss=0.0344]\u001b[0m\n",
      "\u001b[35mSteps:  47%|████▋     | 47/100 [17:29<19:50, 22.46s/it, lr=1e-6, step_loss=0.0885]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 47/100 [17:29<19:50, 22.46s/it, lr=1e-6, step_loss=0.0219]\u001b[0m\n",
      "\u001b[35mSteps:  47%|████▋     | 47/100 [17:34<19:50, 22.46s/it, lr=1e-6, step_loss=0.0483]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 47/100 [17:33<19:50, 22.46s/it, lr=1e-6, step_loss=0.0219]\u001b[0m\n",
      "\u001b[35mSteps:  47%|████▋     | 47/100 [17:38<19:50, 22.46s/it, lr=1e-6, step_loss=0.0722]\u001b[0m\n",
      "\u001b[34mSteps:  47%|████▋     | 47/100 [17:38<19:50, 22.46s/it, lr=1e-6, step_loss=0.0252]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 48/100 [17:42<19:36, 22.62s/it, lr=1e-6, step_loss=0.0252]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 48/100 [17:42<19:36, 22.62s/it, lr=1e-6, step_loss=0.0464]\u001b[0m\n",
      "\u001b[35mSteps:  48%|████▊     | 48/100 [17:43<19:36, 22.62s/it, lr=1e-6, step_loss=0.0722]\u001b[0m\n",
      "\u001b[35mSteps:  48%|████▊     | 48/100 [17:43<19:36, 22.62s/it, lr=1e-6, step_loss=0.0267]\u001b[0m\n",
      "\u001b[35mSteps:  48%|████▊     | 48/100 [17:52<19:36, 22.62s/it, lr=1e-6, step_loss=0.0244]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 48/100 [17:52<19:36, 22.62s/it, lr=1e-6, step_loss=0.0706]\u001b[0m\n",
      "\u001b[35mSteps:  48%|████▊     | 48/100 [17:57<19:36, 22.62s/it, lr=1e-6, step_loss=0.15]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 48/100 [17:56<19:36, 22.62s/it, lr=1e-6, step_loss=0.0234]\u001b[0m\n",
      "\u001b[35mSteps:  48%|████▊     | 48/100 [18:01<19:36, 22.62s/it, lr=1e-6, step_loss=0.0289]\u001b[0m\n",
      "\u001b[34mSteps:  48%|████▊     | 48/100 [18:01<19:36, 22.62s/it, lr=1e-6, step_loss=0.0115]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 49/100 [18:05<19:20, 22.75s/it, lr=1e-6, step_loss=0.0115]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 49/100 [18:05<19:20, 22.75s/it, lr=1e-6, step_loss=0.0134]\u001b[0m\n",
      "\u001b[35mSteps:  49%|████▉     | 49/100 [18:06<19:20, 22.75s/it, lr=1e-6, step_loss=0.0289]\u001b[0m\n",
      "\u001b[35mSteps:  49%|████▉     | 49/100 [18:06<19:20, 22.75s/it, lr=1e-6, step_loss=0.0363]\u001b[0m\n",
      "\u001b[35mSteps:  49%|████▉     | 49/100 [18:15<19:20, 22.75s/it, lr=1e-6, step_loss=0.0726]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 49/100 [18:15<19:20, 22.75s/it, lr=1e-6, step_loss=0.0801]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 49/100 [18:19<19:20, 22.75s/it, lr=1e-6, step_loss=0.0713]\u001b[0m\n",
      "\u001b[35mSteps:  49%|████▉     | 49/100 [18:20<19:20, 22.75s/it, lr=1e-6, step_loss=0.0425]\u001b[0m\n",
      "\u001b[35mSteps:  49%|████▉     | 49/100 [18:24<19:20, 22.75s/it, lr=1e-6, step_loss=0.0789]\u001b[0m\n",
      "\u001b[34mSteps:  49%|████▉     | 49/100 [18:24<19:20, 22.75s/it, lr=1e-6, step_loss=0.0442]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 50/100 [18:28<19:02, 22.86s/it, lr=1e-6, step_loss=0.0442]\u001b[0m\n",
      "\u001b[34m03/22/2024 16:50:06 - INFO - accelerate.accelerator - Saving current state to /opt/ml/model/checkpoint-50\u001b[0m\n",
      "\u001b[34m03/22/2024 16:50:06 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:06,392] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:06,426] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-50/pytorch_model/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:06,426] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[35mSteps:  50%|█████     | 50/100 [18:29<19:02, 22.86s/it, lr=1e-6, step_loss=0.0789]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:12,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_1_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_2_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_3_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_7_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_4_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_5_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:12,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_6_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_7_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,384] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_7_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,384] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_5_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,409] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_5_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_6_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,415] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_6_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,415] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_4_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,418] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_4_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 16:50:16,418] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,081] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,081] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_3_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,331] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_3_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,331] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_1_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,405] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_1_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,405] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_2_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,476] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-50/pytorch_model/zero_pp_rank_2_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 16:50:17,476] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m03/22/2024 16:50:17 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /opt/ml/model/checkpoint-50/pytorch_model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-50/unet/config.json\u001b[0m\n",
      "\u001b[35mSteps:  50%|█████     | 50/100 [18:40<19:02, 22.86s/it, lr=1e-6, step_loss=0.039]\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-50/unet/diffusion_pytorch_model.safetensors\u001b[0m\n",
      "\u001b[34m03/22/2024 16:50:22 - INFO - accelerate.checkpointing - Scheduler state saved in /opt/ml/model/checkpoint-50/scheduler.bin\u001b[0m\n",
      "\u001b[34m03/22/2024 16:50:22 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /opt/ml/model/checkpoint-50/sampler.bin\u001b[0m\n",
      "\u001b[34m03/22/2024 16:50:22 - INFO - accelerate.checkpointing - Random states saved in /opt/ml/model/checkpoint-50/random_states_0.pkl\u001b[0m\n",
      "\u001b[34m03/22/2024 16:50:22 - INFO - __main__ - Saved state to /opt/ml/model/checkpoint-50\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 50/100 [18:45<19:02, 22.86s/it, lr=1e-6, step_loss=0.0708]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 50/100 [18:54<19:02, 22.86s/it, lr=1e-6, step_loss=0.0662]\u001b[0m\n",
      "\u001b[35mSteps:  50%|█████     | 50/100 [18:55<19:02, 22.86s/it, lr=1e-6, step_loss=0.0621]\u001b[0m\n",
      "\u001b[35mSteps:  50%|█████     | 50/100 [19:00<19:02, 22.86s/it, lr=1e-6, step_loss=0.0167]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 50/100 [18:59<19:02, 22.86s/it, lr=1e-6, step_loss=0.0671]\u001b[0m\n",
      "\u001b[35mSteps:  50%|█████     | 50/100 [19:04<19:02, 22.86s/it, lr=1e-6, step_loss=0.063]\u001b[0m\n",
      "\u001b[34mSteps:  50%|█████     | 50/100 [19:04<19:02, 22.86s/it, lr=1e-6, step_loss=0.00368]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 51/100 [19:09<22:53, 28.03s/it, lr=1e-6, step_loss=0.00368]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 51/100 [19:09<22:53, 28.03s/it, lr=1e-6, step_loss=0.00617]\u001b[0m\n",
      "\u001b[35mSteps:  51%|█████     | 51/100 [19:09<22:53, 28.03s/it, lr=1e-6, step_loss=0.063]\u001b[0m\n",
      "\u001b[35mSteps:  51%|█████     | 51/100 [19:09<22:53, 28.03s/it, lr=1e-6, step_loss=0.0353]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 51/100 [19:18<22:53, 28.03s/it, lr=1e-6, step_loss=0.0671]\u001b[0m\n",
      "\u001b[35mSteps:  51%|█████     | 51/100 [19:19<22:53, 28.03s/it, lr=1e-6, step_loss=0.0716]\u001b[0m\n",
      "\u001b[35mSteps:  51%|█████     | 51/100 [19:23<22:53, 28.03s/it, lr=1e-6, step_loss=0.00805]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 51/100 [19:23<22:53, 28.03s/it, lr=1e-6, step_loss=0.0111]\u001b[0m\n",
      "\u001b[35mSteps:  51%|█████     | 51/100 [19:28<22:53, 28.03s/it, lr=1e-6, step_loss=0.0489]\u001b[0m\n",
      "\u001b[34mSteps:  51%|█████     | 51/100 [19:28<22:53, 28.03s/it, lr=1e-6, step_loss=0.0699]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 52/100 [19:32<21:25, 26.78s/it, lr=1e-6, step_loss=0.0699]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 52/100 [19:32<21:25, 26.78s/it, lr=1e-6, step_loss=0.0428]\u001b[0m\n",
      "\u001b[35mSteps:  52%|█████▏    | 52/100 [19:33<21:25, 26.78s/it, lr=1e-6, step_loss=0.0489]\u001b[0m\n",
      "\u001b[35mSteps:  52%|█████▏    | 52/100 [19:33<21:25, 26.78s/it, lr=1e-6, step_loss=0.0364]\u001b[0m\n",
      "\u001b[35mSteps:  52%|█████▏    | 52/100 [19:43<21:25, 26.78s/it, lr=1e-6, step_loss=0.0505]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 52/100 [19:42<21:25, 26.78s/it, lr=1e-6, step_loss=0.0208]\u001b[0m\n",
      "\u001b[35mSteps:  52%|█████▏    | 52/100 [19:47<21:25, 26.78s/it, lr=1e-6, step_loss=0.0156]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 52/100 [19:47<21:25, 26.78s/it, lr=1e-6, step_loss=0.0611]\u001b[0m\n",
      "\u001b[34mSteps:  52%|█████▏    | 52/100 [19:51<21:25, 26.78s/it, lr=1e-6, step_loss=0.0443]\u001b[0m\n",
      "\u001b[35mSteps:  52%|█████▏    | 52/100 [19:52<21:25, 26.78s/it, lr=1e-6, step_loss=0.0715]\u001b[0m\n",
      "\u001b[35mSteps:  53%|█████▎    | 53/100 [19:57<20:17, 25.90s/it, lr=1e-6, step_loss=0.0715]\u001b[0m\n",
      "\u001b[35mSteps:  53%|█████▎    | 53/100 [19:57<20:17, 25.90s/it, lr=1e-6, step_loss=0.0287]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 53/100 [19:56<20:17, 25.90s/it, lr=1e-6, step_loss=0.0443]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 53/100 [19:56<20:17, 25.90s/it, lr=1e-6, step_loss=0.00826]\u001b[0m\n",
      "\u001b[35mSteps:  53%|█████▎    | 53/100 [20:07<20:17, 25.90s/it, lr=1e-6, step_loss=0.0553]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 53/100 [20:06<20:17, 25.90s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 53/100 [20:10<20:17, 25.90s/it, lr=1e-6, step_loss=0.0823]\u001b[0m\n",
      "\u001b[35mSteps:  53%|█████▎    | 53/100 [20:11<20:17, 25.90s/it, lr=1e-6, step_loss=0.055]\u001b[0m\n",
      "\u001b[35mSteps:  53%|█████▎    | 53/100 [20:16<20:17, 25.90s/it, lr=1e-6, step_loss=0.0581]\u001b[0m\n",
      "\u001b[34mSteps:  53%|█████▎    | 53/100 [20:15<20:17, 25.90s/it, lr=1e-6, step_loss=0.0512]\u001b[0m\n",
      "\u001b[35mSteps:  54%|█████▍    | 54/100 [20:21<19:22, 25.28s/it, lr=1e-6, step_loss=0.0581]\u001b[0m\n",
      "\u001b[35mSteps:  54%|█████▍    | 54/100 [20:21<19:22, 25.28s/it, lr=1e-6, step_loss=0.0145]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 54/100 [20:20<19:22, 25.28s/it, lr=1e-6, step_loss=0.0512]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 54/100 [20:20<19:22, 25.28s/it, lr=1e-6, step_loss=0.0554]\u001b[0m\n",
      "\u001b[35mSteps:  54%|█████▍    | 54/100 [20:30<19:22, 25.28s/it, lr=1e-6, step_loss=0.0204]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 54/100 [20:30<19:22, 25.28s/it, lr=1e-6, step_loss=0.0172]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 54/100 [20:34<19:22, 25.28s/it, lr=1e-6, step_loss=0.0549]\u001b[0m\n",
      "\u001b[35mSteps:  54%|█████▍    | 54/100 [20:35<19:22, 25.28s/it, lr=1e-6, step_loss=0.0485]\u001b[0m\n",
      "\u001b[35mSteps:  54%|█████▍    | 54/100 [20:40<19:22, 25.28s/it, lr=1e-6, step_loss=0.0401]\u001b[0m\n",
      "\u001b[34mSteps:  54%|█████▍    | 54/100 [20:39<19:22, 25.28s/it, lr=1e-6, step_loss=0.0454]\u001b[0m\n",
      "\u001b[35mSteps:  55%|█████▌    | 55/100 [20:45<18:38, 24.87s/it, lr=1e-6, step_loss=0.0401]\u001b[0m\n",
      "\u001b[35mSteps:  55%|█████▌    | 55/100 [20:45<18:38, 24.87s/it, lr=1e-6, step_loss=0.0871]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▌    | 55/100 [20:44<18:38, 24.87s/it, lr=1e-6, step_loss=0.0454]\u001b[0m\n",
      "\u001b[34mSteps:  55%|█████▌    | 55/100 [20:44<18:38, 24.87s/it, lr=1e-6, step_loss=0.0115]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 56/100 [20:54<15:02, 20.51s/it, lr=1e-6, step_loss=0.0115]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 56/100 [20:54<15:02, 20.51s/it, lr=1e-6, step_loss=0.0359]\u001b[0m\n",
      "\u001b[35mSteps:  56%|█████▌    | 56/100 [20:55<15:03, 20.53s/it, lr=1e-6, step_loss=0.0871]\u001b[0m\n",
      "\u001b[35mSteps:  56%|█████▌    | 56/100 [20:55<15:03, 20.53s/it, lr=1e-6, step_loss=0.068]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 56/100 [21:01<15:02, 20.51s/it, lr=1e-6, step_loss=0.116]\u001b[0m\n",
      "\u001b[35mSteps:  56%|█████▌    | 56/100 [21:01<15:03, 20.53s/it, lr=1e-6, step_loss=0.0308]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 56/100 [21:05<15:02, 20.51s/it, lr=1e-6, step_loss=0.00808]\u001b[0m\n",
      "\u001b[35mSteps:  56%|█████▌    | 56/100 [21:06<15:03, 20.53s/it, lr=1e-6, step_loss=0.00536]\u001b[0m\n",
      "\u001b[34mSteps:  56%|█████▌    | 56/100 [21:10<15:02, 20.51s/it, lr=1e-6, step_loss=0.0444]\u001b[0m\n",
      "\u001b[35mSteps:  56%|█████▌    | 56/100 [21:11<15:03, 20.53s/it, lr=1e-6, step_loss=0.0623]\u001b[0m\n",
      "\u001b[35mSteps:  57%|█████▋    | 57/100 [21:24<16:28, 22.99s/it, lr=1e-6, step_loss=0.0623]\u001b[0m\n",
      "\u001b[35mSteps:  57%|█████▋    | 57/100 [21:24<16:28, 22.99s/it, lr=1e-6, step_loss=0.0395]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 57/100 [21:23<16:28, 22.98s/it, lr=1e-6, step_loss=0.0444]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 57/100 [21:23<16:28, 22.98s/it, lr=1e-6, step_loss=0.0668]\u001b[0m\n",
      "\u001b[35mSteps:  57%|█████▋    | 57/100 [21:28<16:28, 22.99s/it, lr=1e-6, step_loss=0.0528]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 57/100 [21:28<16:28, 22.98s/it, lr=1e-6, step_loss=0.0708]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 57/100 [21:32<16:28, 22.98s/it, lr=1e-6, step_loss=0.103]\u001b[0m\n",
      "\u001b[35mSteps:  57%|█████▋    | 57/100 [21:33<16:28, 22.99s/it, lr=1e-6, step_loss=0.0115]\u001b[0m\n",
      "\u001b[35mSteps:  57%|█████▋    | 57/100 [21:38<16:28, 22.99s/it, lr=1e-6, step_loss=0.0394]\u001b[0m\n",
      "\u001b[34mSteps:  57%|█████▋    | 57/100 [21:37<16:28, 22.98s/it, lr=1e-6, step_loss=0.0891]\u001b[0m\n",
      "\u001b[35mSteps:  58%|█████▊    | 58/100 [21:48<16:16, 23.26s/it, lr=1e-6, step_loss=0.0394]\u001b[0m\n",
      "\u001b[35mSteps:  58%|█████▊    | 58/100 [21:48<16:16, 23.26s/it, lr=1e-6, step_loss=0.00483]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 58/100 [21:47<16:16, 23.25s/it, lr=1e-6, step_loss=0.0891]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 58/100 [21:47<16:16, 23.25s/it, lr=1e-6, step_loss=0.086]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 58/100 [21:52<16:16, 23.25s/it, lr=1e-6, step_loss=0.00865]\u001b[0m\n",
      "\u001b[35mSteps:  58%|█████▊    | 58/100 [21:52<16:16, 23.26s/it, lr=1e-6, step_loss=0.0698]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 58/100 [21:56<16:16, 23.25s/it, lr=1e-6, step_loss=0.0123]\u001b[0m\n",
      "\u001b[35mSteps:  58%|█████▊    | 58/100 [21:57<16:16, 23.26s/it, lr=1e-6, step_loss=0.0486]\u001b[0m\n",
      "\u001b[35mSteps:  58%|█████▊    | 58/100 [22:02<16:16, 23.26s/it, lr=1e-6, step_loss=0.0614]\u001b[0m\n",
      "\u001b[34mSteps:  58%|█████▊    | 58/100 [22:01<16:16, 23.25s/it, lr=1e-6, step_loss=0.0433]\u001b[0m\n",
      "\u001b[35mSteps:  59%|█████▉    | 59/100 [22:12<16:02, 23.47s/it, lr=1e-6, step_loss=0.0614]\u001b[0m\n",
      "\u001b[35mSteps:  59%|█████▉    | 59/100 [22:12<16:02, 23.47s/it, lr=1e-6, step_loss=0.0468]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 59/100 [22:11<16:02, 23.47s/it, lr=1e-6, step_loss=0.0433]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 59/100 [22:11<16:02, 23.47s/it, lr=1e-6, step_loss=0.048]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 59/100 [22:15<16:02, 23.47s/it, lr=1e-6, step_loss=0.0921]\u001b[0m\n",
      "\u001b[35mSteps:  59%|█████▉    | 59/100 [22:16<16:02, 23.47s/it, lr=1e-6, step_loss=0.0542]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 59/100 [22:20<16:02, 23.47s/it, lr=1e-6, step_loss=0.0298]\u001b[0m\n",
      "\u001b[35mSteps:  59%|█████▉    | 59/100 [22:21<16:02, 23.47s/it, lr=1e-6, step_loss=0.0631]\u001b[0m\n",
      "\u001b[35mSteps:  59%|█████▉    | 59/100 [22:26<16:02, 23.47s/it, lr=1e-6, step_loss=0.0948]\u001b[0m\n",
      "\u001b[34mSteps:  59%|█████▉    | 59/100 [22:25<16:02, 23.47s/it, lr=1e-6, step_loss=0.0362]\u001b[0m\n",
      "\u001b[35mSteps:  60%|██████    | 60/100 [22:35<15:44, 23.60s/it, lr=1e-6, step_loss=0.0948]\u001b[0m\n",
      "\u001b[35mSteps:  60%|██████    | 60/100 [22:35<15:44, 23.60s/it, lr=1e-6, step_loss=0.0252]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 60/100 [22:35<15:43, 23.60s/it, lr=1e-6, step_loss=0.0362]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 60/100 [22:35<15:43, 23.60s/it, lr=1e-6, step_loss=0.0435]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 60/100 [22:39<15:43, 23.60s/it, lr=1e-6, step_loss=0.0708]\u001b[0m\n",
      "\u001b[35mSteps:  60%|██████    | 60/100 [22:40<15:44, 23.60s/it, lr=1e-6, step_loss=0.0483]\u001b[0m\n",
      "\u001b[35mSteps:  60%|██████    | 60/100 [22:45<15:44, 23.60s/it, lr=1e-6, step_loss=0.056]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 60/100 [22:44<15:43, 23.60s/it, lr=1e-6, step_loss=0.126]\u001b[0m\n",
      "\u001b[35mSteps:  60%|██████    | 60/100 [22:50<15:44, 23.60s/it, lr=1e-6, step_loss=0.0862]\u001b[0m\n",
      "\u001b[34mSteps:  60%|██████    | 60/100 [22:49<15:43, 23.60s/it, lr=1e-6, step_loss=0.0654]\u001b[0m\n",
      "\u001b[35mSteps:  61%|██████    | 61/100 [22:59<15:22, 23.66s/it, lr=1e-6, step_loss=0.0862]\u001b[0m\n",
      "\u001b[35mSteps:  61%|██████    | 61/100 [22:59<15:22, 23.66s/it, lr=1e-6, step_loss=0.0646]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 61/100 [22:59<15:22, 23.66s/it, lr=1e-6, step_loss=0.0654]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 61/100 [22:59<15:22, 23.66s/it, lr=1e-6, step_loss=0.0489]\u001b[0m\n",
      "\u001b[35mSteps:  61%|██████    | 61/100 [23:04<15:22, 23.66s/it, lr=1e-6, step_loss=0.0728]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 61/100 [23:03<15:22, 23.66s/it, lr=1e-6, step_loss=0.0568]\u001b[0m\n",
      "\u001b[35mSteps:  61%|██████    | 61/100 [23:09<15:22, 23.66s/it, lr=1e-6, step_loss=0.0158]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 61/100 [23:08<15:22, 23.66s/it, lr=1e-6, step_loss=0.0404]\u001b[0m\n",
      "\u001b[35mSteps:  61%|██████    | 61/100 [23:13<15:22, 23.66s/it, lr=1e-6, step_loss=0.137]\u001b[0m\n",
      "\u001b[34mSteps:  61%|██████    | 61/100 [23:13<15:22, 23.66s/it, lr=1e-6, step_loss=0.0287]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 62/100 [23:22<15:01, 23.72s/it, lr=1e-6, step_loss=0.0287]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 62/100 [23:22<15:01, 23.72s/it, lr=1e-6, step_loss=0.0245]\u001b[0m\n",
      "\u001b[35mSteps:  62%|██████▏   | 62/100 [23:23<15:01, 23.72s/it, lr=1e-6, step_loss=0.137]\u001b[0m\n",
      "\u001b[35mSteps:  62%|██████▏   | 62/100 [23:23<15:01, 23.72s/it, lr=1e-6, step_loss=0.0425]\u001b[0m\n",
      "\u001b[35mSteps:  62%|██████▏   | 62/100 [23:28<15:01, 23.72s/it, lr=1e-6, step_loss=0.0444]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 62/100 [23:27<15:01, 23.72s/it, lr=1e-6, step_loss=0.0578]\u001b[0m\n",
      "\u001b[35mSteps:  62%|██████▏   | 62/100 [23:32<15:01, 23.72s/it, lr=1e-6, step_loss=0.0471]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 62/100 [23:32<15:01, 23.72s/it, lr=1e-6, step_loss=0.0343]\u001b[0m\n",
      "\u001b[35mSteps:  62%|██████▏   | 62/100 [23:37<15:01, 23.72s/it, lr=1e-6, step_loss=0.0611]\u001b[0m\n",
      "\u001b[34mSteps:  62%|██████▏   | 62/100 [23:37<15:01, 23.72s/it, lr=1e-6, step_loss=0.0621]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 63/100 [23:46<14:40, 23.78s/it, lr=1e-6, step_loss=0.0621]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 63/100 [23:46<14:40, 23.78s/it, lr=1e-6, step_loss=0.00721]\u001b[0m\n",
      "\u001b[35mSteps:  63%|██████▎   | 63/100 [23:47<14:40, 23.79s/it, lr=1e-6, step_loss=0.0611]\u001b[0m\n",
      "\u001b[35mSteps:  63%|██████▎   | 63/100 [23:47<14:40, 23.79s/it, lr=1e-6, step_loss=0.057]\u001b[0m\n",
      "\u001b[35mSteps:  63%|██████▎   | 63/100 [23:52<14:40, 23.79s/it, lr=1e-6, step_loss=0.00829]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 63/100 [23:51<14:40, 23.78s/it, lr=1e-6, step_loss=0.0247]\u001b[0m\n",
      "\u001b[35mSteps:  63%|██████▎   | 63/100 [23:56<14:40, 23.79s/it, lr=1e-6, step_loss=0.108]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 63/100 [23:56<14:40, 23.78s/it, lr=1e-6, step_loss=0.0937]\u001b[0m\n",
      "\u001b[35mSteps:  63%|██████▎   | 63/100 [24:01<14:40, 23.79s/it, lr=1e-6, step_loss=0.0485]\u001b[0m\n",
      "\u001b[34mSteps:  63%|██████▎   | 63/100 [24:01<14:40, 23.78s/it, lr=1e-6, step_loss=0.0694]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 64/100 [24:10<14:17, 23.82s/it, lr=1e-6, step_loss=0.0694]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 64/100 [24:10<14:17, 23.82s/it, lr=1e-6, step_loss=0.00862]\u001b[0m\n",
      "\u001b[35mSteps:  64%|██████▍   | 64/100 [24:11<14:17, 23.83s/it, lr=1e-6, step_loss=0.0485]\u001b[0m\n",
      "\u001b[35mSteps:  64%|██████▍   | 64/100 [24:11<14:17, 23.83s/it, lr=1e-6, step_loss=0.0223]\u001b[0m\n",
      "\u001b[35mSteps:  64%|██████▍   | 64/100 [24:15<14:17, 23.83s/it, lr=1e-6, step_loss=0.0425]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 64/100 [24:15<14:17, 23.82s/it, lr=1e-6, step_loss=0.0321]\u001b[0m\n",
      "\u001b[35mSteps:  64%|██████▍   | 64/100 [24:20<14:17, 23.83s/it, lr=1e-6, step_loss=0.0307]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 64/100 [24:20<14:17, 23.82s/it, lr=1e-6, step_loss=0.0944]\u001b[0m\n",
      "\u001b[34mSteps:  64%|██████▍   | 64/100 [24:25<14:17, 23.82s/it, lr=1e-6, step_loss=0.0469]\u001b[0m\n",
      "\u001b[35mSteps:  64%|██████▍   | 64/100 [24:25<14:17, 23.83s/it, lr=1e-6, step_loss=0.0899]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 65/100 [24:34<13:55, 23.86s/it, lr=1e-6, step_loss=0.0469]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 65/100 [24:34<13:55, 23.86s/it, lr=1e-6, step_loss=0.0768]\u001b[0m\n",
      "\u001b[35mSteps:  65%|██████▌   | 65/100 [24:35<13:55, 23.86s/it, lr=1e-6, step_loss=0.0899]\u001b[0m\n",
      "\u001b[35mSteps:  65%|██████▌   | 65/100 [24:35<13:55, 23.86s/it, lr=1e-6, step_loss=0.0495]\u001b[0m\n",
      "\u001b[35mSteps:  65%|██████▌   | 65/100 [24:39<13:55, 23.86s/it, lr=1e-6, step_loss=0.0671]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 65/100 [24:39<13:55, 23.86s/it, lr=1e-6, step_loss=0.0118]\u001b[0m\n",
      "\u001b[35mSteps:  65%|██████▌   | 65/100 [24:44<13:55, 23.86s/it, lr=1e-6, step_loss=0.0712]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 65/100 [24:44<13:55, 23.86s/it, lr=1e-6, step_loss=0.0274]\u001b[0m\n",
      "\u001b[34mSteps:  65%|██████▌   | 65/100 [24:48<13:55, 23.86s/it, lr=1e-6, step_loss=0.0491]\u001b[0m\n",
      "\u001b[35mSteps:  65%|██████▌   | 65/100 [24:49<13:55, 23.86s/it, lr=1e-6, step_loss=0.0421]\u001b[0m\n",
      "\u001b[35mSteps:  66%|██████▌   | 66/100 [24:59<13:32, 23.88s/it, lr=1e-6, step_loss=0.0421]\u001b[0m\n",
      "\u001b[35mSteps:  66%|██████▌   | 66/100 [24:59<13:32, 23.88s/it, lr=1e-6, step_loss=0.0588]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 66/100 [24:58<13:32, 23.88s/it, lr=1e-6, step_loss=0.0491]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 66/100 [24:58<13:32, 23.88s/it, lr=1e-6, step_loss=0.0335]\u001b[0m\n",
      "\u001b[35mSteps:  66%|██████▌   | 66/100 [25:03<13:32, 23.88s/it, lr=1e-6, step_loss=0.0348]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 66/100 [25:03<13:32, 23.88s/it, lr=1e-6, step_loss=0.0682]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 66/100 [25:08<13:32, 23.88s/it, lr=1e-6, step_loss=0.0732]\u001b[0m\n",
      "\u001b[35mSteps:  66%|██████▌   | 66/100 [25:08<13:32, 23.88s/it, lr=1e-6, step_loss=0.0582]\u001b[0m\n",
      "\u001b[34mSteps:  66%|██████▌   | 66/100 [25:12<13:32, 23.88s/it, lr=1e-6, step_loss=0.0845]\u001b[0m\n",
      "\u001b[35mSteps:  66%|██████▌   | 66/100 [25:13<13:32, 23.88s/it, lr=1e-6, step_loss=0.0107]\u001b[0m\n",
      "\u001b[35mSteps:  67%|██████▋   | 67/100 [25:23<13:08, 23.89s/it, lr=1e-6, step_loss=0.0107]\u001b[0m\n",
      "\u001b[35mSteps:  67%|██████▋   | 67/100 [25:23<13:08, 23.89s/it, lr=1e-6, step_loss=0.00472]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 67/100 [25:22<13:08, 23.88s/it, lr=1e-6, step_loss=0.0845]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 67/100 [25:22<13:08, 23.88s/it, lr=1e-6, step_loss=0.0178]\u001b[0m\n",
      "\u001b[35mSteps:  67%|██████▋   | 67/100 [25:27<13:08, 23.89s/it, lr=1e-6, step_loss=0.0297]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 67/100 [25:27<13:08, 23.88s/it, lr=1e-6, step_loss=0.0234]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 67/100 [25:32<13:08, 23.88s/it, lr=1e-6, step_loss=0.0839]\u001b[0m\n",
      "\u001b[35mSteps:  67%|██████▋   | 67/100 [25:32<13:08, 23.89s/it, lr=1e-6, step_loss=0.0175]\u001b[0m\n",
      "\u001b[34mSteps:  67%|██████▋   | 67/100 [25:36<13:08, 23.88s/it, lr=1e-6, step_loss=0.0307]\u001b[0m\n",
      "\u001b[35mSteps:  67%|██████▋   | 67/100 [25:37<13:08, 23.89s/it, lr=1e-6, step_loss=0.00394]\u001b[0m\n",
      "\u001b[35mSteps:  68%|██████▊   | 68/100 [25:47<12:45, 23.93s/it, lr=1e-6, step_loss=0.00394]\u001b[0m\n",
      "\u001b[35mSteps:  68%|██████▊   | 68/100 [25:47<12:45, 23.93s/it, lr=1e-6, step_loss=0.0725]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 68/100 [25:46<12:45, 23.93s/it, lr=1e-6, step_loss=0.0307]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 68/100 [25:46<12:45, 23.93s/it, lr=1e-6, step_loss=0.104]\u001b[0m\n",
      "\u001b[35mSteps:  68%|██████▊   | 68/100 [25:51<12:45, 23.93s/it, lr=1e-6, step_loss=0.0349]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 68/100 [25:51<12:45, 23.93s/it, lr=1e-6, step_loss=0.0745]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 68/100 [25:55<12:45, 23.93s/it, lr=1e-6, step_loss=0.0741]\u001b[0m\n",
      "\u001b[35mSteps:  68%|██████▊   | 68/100 [25:56<12:45, 23.93s/it, lr=1e-6, step_loss=0.0288]\u001b[0m\n",
      "\u001b[35mSteps:  68%|██████▊   | 68/100 [26:01<12:45, 23.93s/it, lr=1e-6, step_loss=0.0566]\u001b[0m\n",
      "\u001b[34mSteps:  68%|██████▊   | 68/100 [26:00<12:45, 23.93s/it, lr=1e-6, step_loss=0.0945]\u001b[0m\n",
      "\u001b[35mSteps:  69%|██████▉   | 69/100 [26:11<12:20, 23.90s/it, lr=1e-6, step_loss=0.0566]\u001b[0m\n",
      "\u001b[35mSteps:  69%|██████▉   | 69/100 [26:11<12:20, 23.90s/it, lr=1e-6, step_loss=0.0644]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 69/100 [26:10<12:20, 23.90s/it, lr=1e-6, step_loss=0.0945]\u001b[0m\n",
      "\u001b[34mSteps:  69%|██████▉   | 69/100 [26:10<12:20, 23.90s/it, lr=1e-6, step_loss=0.0473]\u001b[0m\n",
      "\u001b[35mSteps:  70%|███████   | 70/100 [26:16<09:08, 18.28s/it, lr=1e-6, step_loss=0.0644]\u001b[0m\n",
      "\u001b[35mSteps:  70%|███████   | 70/100 [26:16<09:08, 18.28s/it, lr=1e-6, step_loss=0.0162]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 70/100 [26:15<09:08, 18.29s/it, lr=1e-6, step_loss=0.0473]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 70/100 [26:15<09:08, 18.29s/it, lr=1e-6, step_loss=0.0201]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 70/100 [26:22<09:08, 18.29s/it, lr=1e-6, step_loss=0.0641]\u001b[0m\n",
      "\u001b[35mSteps:  70%|███████   | 70/100 [26:22<09:08, 18.28s/it, lr=1e-6, step_loss=0.0437]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 70/100 [26:26<09:08, 18.29s/it, lr=1e-6, step_loss=0.00451]\u001b[0m\n",
      "\u001b[35mSteps:  70%|███████   | 70/100 [26:27<09:08, 18.28s/it, lr=1e-6, step_loss=0.0248]\u001b[0m\n",
      "\u001b[35mSteps:  70%|███████   | 70/100 [26:40<09:08, 18.28s/it, lr=1e-6, step_loss=0.0739]\u001b[0m\n",
      "\u001b[34mSteps:  70%|███████   | 70/100 [26:39<09:08, 18.29s/it, lr=1e-6, step_loss=0.0258]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 71/100 [26:44<10:18, 21.32s/it, lr=1e-6, step_loss=0.0258]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 71/100 [26:44<10:18, 21.32s/it, lr=1e-6, step_loss=0.0622]\u001b[0m\n",
      "\u001b[35mSteps:  71%|███████   | 71/100 [26:44<10:18, 21.31s/it, lr=1e-6, step_loss=0.0739]\u001b[0m\n",
      "\u001b[35mSteps:  71%|███████   | 71/100 [26:44<10:18, 21.31s/it, lr=1e-6, step_loss=0.0966]\u001b[0m\n",
      "\u001b[35mSteps:  71%|███████   | 71/100 [26:49<10:18, 21.31s/it, lr=1e-6, step_loss=0.0527]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 71/100 [26:48<10:18, 21.32s/it, lr=1e-6, step_loss=0.0821]\u001b[0m\n",
      "\u001b[35mSteps:  71%|███████   | 71/100 [26:54<10:18, 21.31s/it, lr=1e-6, step_loss=0.0688]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 71/100 [26:53<10:18, 21.32s/it, lr=1e-6, step_loss=0.068]\u001b[0m\n",
      "\u001b[35mSteps:  71%|███████   | 71/100 [27:04<10:18, 21.31s/it, lr=1e-6, step_loss=0.0343]\u001b[0m\n",
      "\u001b[34mSteps:  71%|███████   | 71/100 [27:03<10:18, 21.32s/it, lr=1e-6, step_loss=0.0359]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 72/100 [27:07<10:18, 22.09s/it, lr=1e-6, step_loss=0.0359]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 72/100 [27:07<10:18, 22.09s/it, lr=1e-6, step_loss=0.103]\u001b[0m\n",
      "\u001b[35mSteps:  72%|███████▏  | 72/100 [27:08<10:18, 22.09s/it, lr=1e-6, step_loss=0.0343]\u001b[0m\n",
      "\u001b[35mSteps:  72%|███████▏  | 72/100 [27:08<10:18, 22.09s/it, lr=1e-6, step_loss=0.0497]\u001b[0m\n",
      "\u001b[35mSteps:  72%|███████▏  | 72/100 [27:13<10:18, 22.09s/it, lr=1e-6, step_loss=0.0153]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 72/100 [27:12<10:18, 22.09s/it, lr=1e-6, step_loss=0.0523]\u001b[0m\n",
      "\u001b[35mSteps:  72%|███████▏  | 72/100 [27:18<10:18, 22.09s/it, lr=1e-6, step_loss=0.0199]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 72/100 [27:17<10:18, 22.09s/it, lr=1e-6, step_loss=0.00708]\u001b[0m\n",
      "\u001b[35mSteps:  72%|███████▏  | 72/100 [27:27<10:18, 22.09s/it, lr=1e-6, step_loss=0.0503]\u001b[0m\n",
      "\u001b[34mSteps:  72%|███████▏  | 72/100 [27:27<10:18, 22.09s/it, lr=1e-6, step_loss=0.0393]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 73/100 [27:31<10:11, 22.64s/it, lr=1e-6, step_loss=0.0393]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 73/100 [27:31<10:11, 22.64s/it, lr=1e-6, step_loss=0.0146]\u001b[0m\n",
      "\u001b[35mSteps:  73%|███████▎  | 73/100 [27:32<10:11, 22.64s/it, lr=1e-6, step_loss=0.0503]\u001b[0m\n",
      "\u001b[35mSteps:  73%|███████▎  | 73/100 [27:32<10:11, 22.64s/it, lr=1e-6, step_loss=0.103]\u001b[0m\n",
      "\u001b[35mSteps:  73%|███████▎  | 73/100 [27:37<10:11, 22.64s/it, lr=1e-6, step_loss=0.0328]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 73/100 [27:36<10:11, 22.64s/it, lr=1e-6, step_loss=0.052]\u001b[0m\n",
      "\u001b[35mSteps:  73%|███████▎  | 73/100 [27:42<10:11, 22.64s/it, lr=1e-6, step_loss=0.0467]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 73/100 [27:41<10:11, 22.64s/it, lr=1e-6, step_loss=0.0702]\u001b[0m\n",
      "\u001b[34mSteps:  73%|███████▎  | 73/100 [27:51<10:11, 22.64s/it, lr=1e-6, step_loss=0.0475]\u001b[0m\n",
      "\u001b[35mSteps:  73%|███████▎  | 73/100 [27:51<10:11, 22.64s/it, lr=1e-6, step_loss=0.118]\u001b[0m\n",
      "\u001b[35mSteps:  74%|███████▍  | 74/100 [27:56<09:57, 22.99s/it, lr=1e-6, step_loss=0.118]\u001b[0m\n",
      "\u001b[35mSteps:  74%|███████▍  | 74/100 [27:56<09:57, 22.99s/it, lr=1e-6, step_loss=0.0449]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 74/100 [27:55<09:57, 22.99s/it, lr=1e-6, step_loss=0.0475]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 74/100 [27:55<09:57, 22.99s/it, lr=1e-6, step_loss=0.102]\u001b[0m\n",
      "\u001b[35mSteps:  74%|███████▍  | 74/100 [28:00<09:57, 22.99s/it, lr=1e-6, step_loss=0.0661]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 74/100 [28:00<09:57, 22.99s/it, lr=1e-6, step_loss=0.0332]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 74/100 [28:04<09:57, 22.99s/it, lr=1e-6, step_loss=0.0685]\u001b[0m\n",
      "\u001b[35mSteps:  74%|███████▍  | 74/100 [28:05<09:57, 22.99s/it, lr=1e-6, step_loss=0.035]\u001b[0m\n",
      "\u001b[35mSteps:  74%|███████▍  | 74/100 [28:15<09:57, 22.99s/it, lr=1e-6, step_loss=0.035]\u001b[0m\n",
      "\u001b[34mSteps:  74%|███████▍  | 74/100 [28:14<09:57, 22.99s/it, lr=1e-6, step_loss=0.09]\u001b[0m\n",
      "\u001b[35mSteps:  75%|███████▌  | 75/100 [28:19<09:35, 23.02s/it, lr=1e-6, step_loss=0.035]\u001b[0m\n",
      "\u001b[35mSteps:  75%|███████▌  | 75/100 [28:19<09:35, 23.02s/it, lr=1e-6, step_loss=0.017]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 75/100 [28:18<09:35, 23.02s/it, lr=1e-6, step_loss=0.09]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 75/100 [28:18<09:35, 23.02s/it, lr=1e-6, step_loss=0.092]\u001b[0m\n",
      "\u001b[35mSteps:  75%|███████▌  | 75/100 [28:23<09:35, 23.02s/it, lr=1e-6, step_loss=0.0353]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 75/100 [28:23<09:35, 23.02s/it, lr=1e-6, step_loss=0.108]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 75/100 [28:27<09:35, 23.02s/it, lr=1e-6, step_loss=0.0415]\u001b[0m\n",
      "\u001b[35mSteps:  75%|███████▌  | 75/100 [28:28<09:35, 23.02s/it, lr=1e-6, step_loss=0.0388]\u001b[0m\n",
      "\u001b[35mSteps:  75%|███████▌  | 75/100 [28:38<09:35, 23.02s/it, lr=1e-6, step_loss=0.0709]\u001b[0m\n",
      "\u001b[34mSteps:  75%|███████▌  | 75/100 [28:37<09:35, 23.02s/it, lr=1e-6, step_loss=0.0284]\u001b[0m\n",
      "\u001b[35mSteps:  76%|███████▌  | 76/100 [28:42<09:12, 23.02s/it, lr=1e-6, step_loss=0.0709]\u001b[0m\n",
      "\u001b[35mSteps:  76%|███████▌  | 76/100 [28:42<09:12, 23.02s/it, lr=1e-6, step_loss=0.0284]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 76/100 [28:41<09:12, 23.02s/it, lr=1e-6, step_loss=0.0284]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 76/100 [28:41<09:12, 23.02s/it, lr=1e-6, step_loss=0.058]\u001b[0m\n",
      "\u001b[35mSteps:  76%|███████▌  | 76/100 [28:46<09:12, 23.02s/it, lr=1e-6, step_loss=0.0774]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 76/100 [28:46<09:12, 23.02s/it, lr=1e-6, step_loss=0.00426]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 76/100 [28:50<09:12, 23.02s/it, lr=1e-6, step_loss=0.0454]\u001b[0m\n",
      "\u001b[35mSteps:  76%|███████▌  | 76/100 [28:51<09:12, 23.02s/it, lr=1e-6, step_loss=0.0594]\u001b[0m\n",
      "\u001b[35mSteps:  76%|███████▌  | 76/100 [29:01<09:12, 23.02s/it, lr=1e-6, step_loss=0.0935]\u001b[0m\n",
      "\u001b[34mSteps:  76%|███████▌  | 76/100 [29:00<09:12, 23.02s/it, lr=1e-6, step_loss=0.0287]\u001b[0m\n",
      "\u001b[35mSteps:  77%|███████▋  | 77/100 [29:05<08:49, 23.03s/it, lr=1e-6, step_loss=0.0935]\u001b[0m\n",
      "\u001b[35mSteps:  77%|███████▋  | 77/100 [29:05<08:49, 23.03s/it, lr=1e-6, step_loss=0.0174]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 77/100 [29:04<08:49, 23.03s/it, lr=1e-6, step_loss=0.0287]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 77/100 [29:04<08:49, 23.03s/it, lr=1e-6, step_loss=0.0218]\u001b[0m\n",
      "\u001b[35mSteps:  77%|███████▋  | 77/100 [29:10<08:49, 23.03s/it, lr=1e-6, step_loss=0.113]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 77/100 [29:09<08:49, 23.03s/it, lr=1e-6, step_loss=0.0132]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 77/100 [29:14<08:49, 23.03s/it, lr=1e-6, step_loss=0.0936]\u001b[0m\n",
      "\u001b[35mSteps:  77%|███████▋  | 77/100 [29:14<08:49, 23.03s/it, lr=1e-6, step_loss=0.0261]\u001b[0m\n",
      "\u001b[35mSteps:  77%|███████▋  | 77/100 [29:24<08:49, 23.03s/it, lr=1e-6, step_loss=0.0117]\u001b[0m\n",
      "\u001b[34mSteps:  77%|███████▋  | 77/100 [29:23<08:49, 23.03s/it, lr=1e-6, step_loss=0.0578]\u001b[0m\n",
      "\u001b[35mSteps:  78%|███████▊  | 78/100 [29:28<08:26, 23.03s/it, lr=1e-6, step_loss=0.0117]\u001b[0m\n",
      "\u001b[35mSteps:  78%|███████▊  | 78/100 [29:28<08:26, 23.03s/it, lr=1e-6, step_loss=0.0222]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 78/100 [29:27<08:26, 23.04s/it, lr=1e-6, step_loss=0.0578]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 78/100 [29:27<08:26, 23.04s/it, lr=1e-6, step_loss=0.0678]\u001b[0m\n",
      "\u001b[35mSteps:  78%|███████▊  | 78/100 [29:33<08:26, 23.03s/it, lr=1e-6, step_loss=0.0312]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 78/100 [29:32<08:26, 23.04s/it, lr=1e-6, step_loss=0.0929]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 78/100 [29:37<08:26, 23.04s/it, lr=1e-6, step_loss=0.0393]\u001b[0m\n",
      "\u001b[35mSteps:  78%|███████▊  | 78/100 [29:37<08:26, 23.03s/it, lr=1e-6, step_loss=0.0868]\u001b[0m\n",
      "\u001b[35mSteps:  78%|███████▊  | 78/100 [29:47<08:26, 23.03s/it, lr=1e-6, step_loss=0.0916]\u001b[0m\n",
      "\u001b[34mSteps:  78%|███████▊  | 78/100 [29:46<08:26, 23.04s/it, lr=1e-6, step_loss=0.0413]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 79/100 [29:50<08:03, 23.03s/it, lr=1e-6, step_loss=0.0413]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 79/100 [29:50<08:03, 23.03s/it, lr=1e-6, step_loss=0.0086]\u001b[0m\n",
      "\u001b[35mSteps:  79%|███████▉  | 79/100 [29:51<08:03, 23.03s/it, lr=1e-6, step_loss=0.0916]\u001b[0m\n",
      "\u001b[35mSteps:  79%|███████▉  | 79/100 [29:51<08:03, 23.03s/it, lr=1e-6, step_loss=0.0223]\u001b[0m\n",
      "\u001b[35mSteps:  79%|███████▉  | 79/100 [29:56<08:03, 23.03s/it, lr=1e-6, step_loss=0.0451]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 79/100 [29:55<08:03, 23.03s/it, lr=1e-6, step_loss=0.0232]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 79/100 [30:00<08:03, 23.03s/it, lr=1e-6, step_loss=0.0509]\u001b[0m\n",
      "\u001b[35mSteps:  79%|███████▉  | 79/100 [30:00<08:03, 23.03s/it, lr=1e-6, step_loss=0.0932]\u001b[0m\n",
      "\u001b[35mSteps:  79%|███████▉  | 79/100 [30:10<08:03, 23.03s/it, lr=1e-6, step_loss=0.085]\u001b[0m\n",
      "\u001b[34mSteps:  79%|███████▉  | 79/100 [30:09<08:03, 23.03s/it, lr=1e-6, step_loss=0.0654]\u001b[0m\n",
      "\u001b[35mSteps:  80%|████████  | 80/100 [30:14<07:40, 23.02s/it, lr=1e-6, step_loss=0.085]\u001b[0m\n",
      "\u001b[35mSteps:  80%|████████  | 80/100 [30:14<07:40, 23.02s/it, lr=1e-6, step_loss=0.0995]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 80/100 [30:13<07:40, 23.02s/it, lr=1e-6, step_loss=0.0654]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 80/100 [30:13<07:40, 23.02s/it, lr=1e-6, step_loss=0.134]\u001b[0m\n",
      "\u001b[35mSteps:  80%|████████  | 80/100 [30:19<07:40, 23.02s/it, lr=1e-6, step_loss=0.0176]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 80/100 [30:18<07:40, 23.02s/it, lr=1e-6, step_loss=0.0156]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 80/100 [30:23<07:40, 23.02s/it, lr=1e-6, step_loss=0.128]\u001b[0m\n",
      "\u001b[35mSteps:  80%|████████  | 80/100 [30:23<07:40, 23.02s/it, lr=1e-6, step_loss=0.0423]\u001b[0m\n",
      "\u001b[35mSteps:  80%|████████  | 80/100 [30:33<07:40, 23.02s/it, lr=1e-6, step_loss=0.09]\u001b[0m\n",
      "\u001b[34mSteps:  80%|████████  | 80/100 [30:32<07:40, 23.02s/it, lr=1e-6, step_loss=0.0657]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 81/100 [30:36<07:17, 23.03s/it, lr=1e-6, step_loss=0.0657]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 81/100 [30:36<07:17, 23.03s/it, lr=1e-6, step_loss=0.0146]\u001b[0m\n",
      "\u001b[35mSteps:  81%|████████  | 81/100 [30:37<07:17, 23.03s/it, lr=1e-6, step_loss=0.09]\u001b[0m\n",
      "\u001b[35mSteps:  81%|████████  | 81/100 [30:37<07:17, 23.03s/it, lr=1e-6, step_loss=0.0437]\u001b[0m\n",
      "\u001b[35mSteps:  81%|████████  | 81/100 [30:42<07:17, 23.03s/it, lr=1e-6, step_loss=0.0512]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 81/100 [30:41<07:17, 23.03s/it, lr=1e-6, step_loss=0.0325]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 81/100 [30:46<07:17, 23.03s/it, lr=1e-6, step_loss=0.00853]\u001b[0m\n",
      "\u001b[35mSteps:  81%|████████  | 81/100 [30:46<07:17, 23.03s/it, lr=1e-6, step_loss=0.0937]\u001b[0m\n",
      "\u001b[35mSteps:  81%|████████  | 81/100 [30:56<07:17, 23.03s/it, lr=1e-6, step_loss=0.0793]\u001b[0m\n",
      "\u001b[34mSteps:  81%|████████  | 81/100 [30:55<07:17, 23.03s/it, lr=1e-6, step_loss=0.0932]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 82/100 [31:00<06:54, 23.05s/it, lr=1e-6, step_loss=0.0932]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 82/100 [31:00<06:54, 23.05s/it, lr=1e-6, step_loss=0.088]\u001b[0m\n",
      "\u001b[35mSteps:  82%|████████▏ | 82/100 [31:00<06:54, 23.05s/it, lr=1e-6, step_loss=0.0793]\u001b[0m\n",
      "\u001b[35mSteps:  82%|████████▏ | 82/100 [31:00<06:54, 23.05s/it, lr=1e-6, step_loss=0.0191]\u001b[0m\n",
      "\u001b[35mSteps:  82%|████████▏ | 82/100 [31:05<06:54, 23.05s/it, lr=1e-6, step_loss=0.0306]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 82/100 [31:04<06:54, 23.05s/it, lr=1e-6, step_loss=0.0208]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 82/100 [31:09<06:54, 23.05s/it, lr=1e-6, step_loss=0.0165]\u001b[0m\n",
      "\u001b[35mSteps:  82%|████████▏ | 82/100 [31:09<06:54, 23.05s/it, lr=1e-6, step_loss=0.105]\u001b[0m\n",
      "\u001b[35mSteps:  82%|████████▏ | 82/100 [31:19<06:54, 23.05s/it, lr=1e-6, step_loss=0.0111]\u001b[0m\n",
      "\u001b[34mSteps:  82%|████████▏ | 82/100 [31:18<06:54, 23.05s/it, lr=1e-6, step_loss=0.0976]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 83/100 [31:23<06:31, 23.05s/it, lr=1e-6, step_loss=0.0976]\u001b[0m\n",
      "\u001b[34mSteps:  83%|████████▎ | 83/100 [31:23<06:31, 23.05s/it, lr=1e-6, step_loss=0.0817]\u001b[0m\n",
      "\u001b[35mSteps:  83%|████████▎ | 83/100 [31:23<06:31, 23.05s/it, lr=1e-6, step_loss=0.0111]\u001b[0m\n",
      "\u001b[35mSteps:  83%|████████▎ | 83/100 [31:23<06:31, 23.05s/it, lr=1e-6, step_loss=0.0917]\u001b[0m\n",
      "\u001b[35mSteps:  84%|████████▍ | 84/100 [31:28<04:43, 17.70s/it, lr=1e-6, step_loss=0.0917]\u001b[0m\n",
      "\u001b[35mSteps:  84%|████████▍ | 84/100 [31:28<04:43, 17.70s/it, lr=1e-6, step_loss=0.0291]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 84/100 [31:28<04:43, 17.71s/it, lr=1e-6, step_loss=0.0817]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 84/100 [31:28<04:43, 17.71s/it, lr=1e-6, step_loss=0.0605]\u001b[0m\n",
      "\u001b[35mSteps:  84%|████████▍ | 84/100 [31:34<04:43, 17.70s/it, lr=1e-6, step_loss=0.0507]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 84/100 [31:34<04:43, 17.71s/it, lr=1e-6, step_loss=0.0294]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 84/100 [31:47<04:43, 17.71s/it, lr=1e-6, step_loss=0.0385]\u001b[0m\n",
      "\u001b[35mSteps:  84%|████████▍ | 84/100 [31:47<04:43, 17.70s/it, lr=1e-6, step_loss=0.0875]\u001b[0m\n",
      "\u001b[35mSteps:  84%|████████▍ | 84/100 [31:51<04:43, 17.70s/it, lr=1e-6, step_loss=0.0189]\u001b[0m\n",
      "\u001b[34mSteps:  84%|████████▍ | 84/100 [31:51<04:43, 17.71s/it, lr=1e-6, step_loss=0.0629]\u001b[0m\n",
      "\u001b[35mSteps:  85%|████████▌ | 85/100 [31:56<05:10, 20.68s/it, lr=1e-6, step_loss=0.0189]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 85/100 [31:55<05:10, 20.68s/it, lr=1e-6, step_loss=0.0629]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 85/100 [31:55<05:10, 20.68s/it, lr=1e-6, step_loss=0.0844]\u001b[0m\n",
      "\u001b[35mSteps:  85%|████████▌ | 85/100 [31:56<05:10, 20.68s/it, lr=1e-6, step_loss=0.111]\u001b[0m\n",
      "\u001b[35mSteps:  85%|████████▌ | 85/100 [32:01<05:10, 20.68s/it, lr=1e-6, step_loss=0.0537]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 85/100 [32:00<05:10, 20.68s/it, lr=1e-6, step_loss=0.03]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 85/100 [32:09<05:10, 20.68s/it, lr=1e-6, step_loss=0.0153]\u001b[0m\n",
      "\u001b[35mSteps:  85%|████████▌ | 85/100 [32:10<05:10, 20.68s/it, lr=1e-6, step_loss=0.0329]\u001b[0m\n",
      "\u001b[35mSteps:  85%|████████▌ | 85/100 [32:14<05:10, 20.68s/it, lr=1e-6, step_loss=0.0593]\u001b[0m\n",
      "\u001b[34mSteps:  85%|████████▌ | 85/100 [32:14<05:10, 20.68s/it, lr=1e-6, step_loss=0.0455]\u001b[0m\n",
      "\u001b[35mSteps:  86%|████████▌ | 86/100 [32:19<04:58, 21.35s/it, lr=1e-6, step_loss=0.0593]\u001b[0m\n",
      "\u001b[35mSteps:  86%|████████▌ | 86/100 [32:19<04:58, 21.35s/it, lr=1e-6, step_loss=0.00566]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 86/100 [32:18<04:58, 21.36s/it, lr=1e-6, step_loss=0.0455]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 86/100 [32:18<04:58, 21.36s/it, lr=1e-6, step_loss=0.0573]\u001b[0m\n",
      "\u001b[35mSteps:  86%|████████▌ | 86/100 [32:24<04:58, 21.35s/it, lr=1e-6, step_loss=0.0449]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 86/100 [32:23<04:58, 21.36s/it, lr=1e-6, step_loss=0.0424]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 86/100 [32:33<04:58, 21.36s/it, lr=1e-6, step_loss=0.0666]\u001b[0m\n",
      "\u001b[35mSteps:  86%|████████▌ | 86/100 [32:33<04:58, 21.35s/it, lr=1e-6, step_loss=0.0923]\u001b[0m\n",
      "\u001b[35mSteps:  86%|████████▌ | 86/100 [32:38<04:58, 21.35s/it, lr=1e-6, step_loss=0.039]\u001b[0m\n",
      "\u001b[34mSteps:  86%|████████▌ | 86/100 [32:37<04:58, 21.36s/it, lr=1e-6, step_loss=0.0938]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 87/100 [32:42<04:44, 21.90s/it, lr=1e-6, step_loss=0.0938]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 87/100 [32:42<04:44, 21.90s/it, lr=1e-6, step_loss=0.118]\u001b[0m\n",
      "\u001b[35mSteps:  87%|████████▋ | 87/100 [32:42<04:44, 21.90s/it, lr=1e-6, step_loss=0.039]\u001b[0m\n",
      "\u001b[35mSteps:  87%|████████▋ | 87/100 [32:42<04:44, 21.90s/it, lr=1e-6, step_loss=0.0285]\u001b[0m\n",
      "\u001b[35mSteps:  87%|████████▋ | 87/100 [32:47<04:44, 21.90s/it, lr=1e-6, step_loss=0.015]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 87/100 [32:46<04:44, 21.90s/it, lr=1e-6, step_loss=0.113]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 87/100 [32:56<04:44, 21.90s/it, lr=1e-6, step_loss=0.0413]\u001b[0m\n",
      "\u001b[35mSteps:  87%|████████▋ | 87/100 [32:56<04:44, 21.90s/it, lr=1e-6, step_loss=0.0645]\u001b[0m\n",
      "\u001b[35mSteps:  87%|████████▋ | 87/100 [33:01<04:44, 21.90s/it, lr=1e-6, step_loss=0.0238]\u001b[0m\n",
      "\u001b[34mSteps:  87%|████████▋ | 87/100 [33:00<04:44, 21.90s/it, lr=1e-6, step_loss=0.0291]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 88/100 [33:05<04:26, 22.24s/it, lr=1e-6, step_loss=0.0291]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 88/100 [33:05<04:26, 22.24s/it, lr=1e-6, step_loss=0.0511]\u001b[0m\n",
      "\u001b[35mSteps:  88%|████████▊ | 88/100 [33:05<04:26, 22.24s/it, lr=1e-6, step_loss=0.0238]\u001b[0m\n",
      "\u001b[35mSteps:  88%|████████▊ | 88/100 [33:05<04:26, 22.24s/it, lr=1e-6, step_loss=0.0578]\u001b[0m\n",
      "\u001b[35mSteps:  88%|████████▊ | 88/100 [33:10<04:26, 22.24s/it, lr=1e-6, step_loss=0.0104]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 88/100 [33:09<04:26, 22.24s/it, lr=1e-6, step_loss=0.038]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 88/100 [33:19<04:26, 22.24s/it, lr=1e-6, step_loss=0.0932]\u001b[0m\n",
      "\u001b[35mSteps:  88%|████████▊ | 88/100 [33:19<04:26, 22.24s/it, lr=1e-6, step_loss=0.0401]\u001b[0m\n",
      "\u001b[35mSteps:  88%|████████▊ | 88/100 [33:24<04:26, 22.24s/it, lr=1e-6, step_loss=0.0574]\u001b[0m\n",
      "\u001b[34mSteps:  88%|████████▊ | 88/100 [33:23<04:26, 22.24s/it, lr=1e-6, step_loss=0.0759]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 89/100 [33:28<04:07, 22.46s/it, lr=1e-6, step_loss=0.0759]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 89/100 [33:28<04:07, 22.46s/it, lr=1e-6, step_loss=0.0822]\u001b[0m\n",
      "\u001b[35mSteps:  89%|████████▉ | 89/100 [33:28<04:07, 22.46s/it, lr=1e-6, step_loss=0.0574]\u001b[0m\n",
      "\u001b[35mSteps:  89%|████████▉ | 89/100 [33:28<04:07, 22.46s/it, lr=1e-6, step_loss=0.0694]\u001b[0m\n",
      "\u001b[35mSteps:  89%|████████▉ | 89/100 [33:33<04:07, 22.46s/it, lr=1e-6, step_loss=0.0823]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 89/100 [33:32<04:07, 22.46s/it, lr=1e-6, step_loss=0.0167]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 89/100 [33:42<04:07, 22.46s/it, lr=1e-6, step_loss=0.00877]\u001b[0m\n",
      "\u001b[35mSteps:  89%|████████▉ | 89/100 [33:42<04:07, 22.46s/it, lr=1e-6, step_loss=0.0745]\u001b[0m\n",
      "\u001b[35mSteps:  89%|████████▉ | 89/100 [33:47<04:07, 22.46s/it, lr=1e-6, step_loss=0.0263]\u001b[0m\n",
      "\u001b[34mSteps:  89%|████████▉ | 89/100 [33:46<04:07, 22.46s/it, lr=1e-6, step_loss=0.0194]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 90/100 [33:51<03:46, 22.62s/it, lr=1e-6, step_loss=0.0194]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 90/100 [33:51<03:46, 22.62s/it, lr=1e-6, step_loss=0.015]\u001b[0m\n",
      "\u001b[35mSteps:  90%|█████████ | 90/100 [33:51<03:46, 22.62s/it, lr=1e-6, step_loss=0.0263]\u001b[0m\n",
      "\u001b[35mSteps:  90%|█████████ | 90/100 [33:51<03:46, 22.62s/it, lr=1e-6, step_loss=0.0223]\u001b[0m\n",
      "\u001b[35mSteps:  90%|█████████ | 90/100 [33:56<03:46, 22.62s/it, lr=1e-6, step_loss=0.0461]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 90/100 [33:55<03:46, 22.62s/it, lr=1e-6, step_loss=0.0374]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 90/100 [34:05<03:46, 22.62s/it, lr=1e-6, step_loss=0.0622]\u001b[0m\n",
      "\u001b[35mSteps:  90%|█████████ | 90/100 [34:05<03:46, 22.62s/it, lr=1e-6, step_loss=0.00403]\u001b[0m\n",
      "\u001b[35mSteps:  90%|█████████ | 90/100 [34:10<03:46, 22.62s/it, lr=1e-6, step_loss=0.0877]\u001b[0m\n",
      "\u001b[34mSteps:  90%|█████████ | 90/100 [34:09<03:46, 22.62s/it, lr=1e-6, step_loss=0.0037]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 91/100 [34:14<03:24, 22.76s/it, lr=1e-6, step_loss=0.0037]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 91/100 [34:14<03:24, 22.76s/it, lr=1e-6, step_loss=0.037]\u001b[0m\n",
      "\u001b[35mSteps:  91%|█████████ | 91/100 [34:14<03:24, 22.76s/it, lr=1e-6, step_loss=0.0877]\u001b[0m\n",
      "\u001b[35mSteps:  91%|█████████ | 91/100 [34:14<03:24, 22.76s/it, lr=1e-6, step_loss=0.0451]\u001b[0m\n",
      "\u001b[35mSteps:  91%|█████████ | 91/100 [34:19<03:24, 22.76s/it, lr=1e-6, step_loss=0.036]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 91/100 [34:18<03:24, 22.76s/it, lr=1e-6, step_loss=0.0314]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 91/100 [34:28<03:24, 22.76s/it, lr=1e-6, step_loss=0.0258]\u001b[0m\n",
      "\u001b[35mSteps:  91%|█████████ | 91/100 [34:28<03:24, 22.76s/it, lr=1e-6, step_loss=0.0569]\u001b[0m\n",
      "\u001b[35mSteps:  91%|█████████ | 91/100 [34:33<03:24, 22.76s/it, lr=1e-6, step_loss=0.0753]\u001b[0m\n",
      "\u001b[34mSteps:  91%|█████████ | 91/100 [34:32<03:24, 22.76s/it, lr=1e-6, step_loss=0.074]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 92/100 [34:37<03:02, 22.85s/it, lr=1e-6, step_loss=0.074]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 92/100 [34:37<03:02, 22.85s/it, lr=1e-6, step_loss=0.0468]\u001b[0m\n",
      "\u001b[35mSteps:  92%|█████████▏| 92/100 [34:37<03:02, 22.85s/it, lr=1e-6, step_loss=0.0753]\u001b[0m\n",
      "\u001b[35mSteps:  92%|█████████▏| 92/100 [34:37<03:02, 22.85s/it, lr=1e-6, step_loss=0.0401]\u001b[0m\n",
      "\u001b[35mSteps:  92%|█████████▏| 92/100 [34:42<03:02, 22.85s/it, lr=1e-6, step_loss=0.0635]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 92/100 [34:41<03:02, 22.85s/it, lr=1e-6, step_loss=0.0798]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 92/100 [34:51<03:02, 22.85s/it, lr=1e-6, step_loss=0.112]\u001b[0m\n",
      "\u001b[35mSteps:  92%|█████████▏| 92/100 [34:51<03:02, 22.85s/it, lr=1e-6, step_loss=0.108]\u001b[0m\n",
      "\u001b[35mSteps:  92%|█████████▏| 92/100 [34:56<03:02, 22.85s/it, lr=1e-6, step_loss=0.0778]\u001b[0m\n",
      "\u001b[34mSteps:  92%|█████████▏| 92/100 [34:55<03:02, 22.85s/it, lr=1e-6, step_loss=0.0897]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 93/100 [35:00<02:40, 22.90s/it, lr=1e-6, step_loss=0.0897]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 93/100 [35:00<02:40, 22.90s/it, lr=1e-6, step_loss=0.0699]\u001b[0m\n",
      "\u001b[35mSteps:  93%|█████████▎| 93/100 [35:00<02:40, 22.90s/it, lr=1e-6, step_loss=0.0778]\u001b[0m\n",
      "\u001b[35mSteps:  93%|█████████▎| 93/100 [35:00<02:40, 22.90s/it, lr=1e-6, step_loss=0.0347]\u001b[0m\n",
      "\u001b[35mSteps:  93%|█████████▎| 93/100 [35:05<02:40, 22.90s/it, lr=1e-6, step_loss=0.0507]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 93/100 [35:04<02:40, 22.90s/it, lr=1e-6, step_loss=0.0272]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 93/100 [35:14<02:40, 22.90s/it, lr=1e-6, step_loss=0.0962]\u001b[0m\n",
      "\u001b[35mSteps:  93%|█████████▎| 93/100 [35:14<02:40, 22.90s/it, lr=1e-6, step_loss=0.0483]\u001b[0m\n",
      "\u001b[35mSteps:  93%|█████████▎| 93/100 [35:19<02:40, 22.90s/it, lr=1e-6, step_loss=0.0672]\u001b[0m\n",
      "\u001b[34mSteps:  93%|█████████▎| 93/100 [35:18<02:40, 22.90s/it, lr=1e-6, step_loss=0.0134]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 94/100 [35:23<02:17, 22.92s/it, lr=1e-6, step_loss=0.0134]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 94/100 [35:23<02:17, 22.92s/it, lr=1e-6, step_loss=0.09]\u001b[0m\n",
      "\u001b[35mSteps:  94%|█████████▍| 94/100 [35:23<02:17, 22.92s/it, lr=1e-6, step_loss=0.0672]\u001b[0m\n",
      "\u001b[35mSteps:  94%|█████████▍| 94/100 [35:23<02:17, 22.92s/it, lr=1e-6, step_loss=0.0377]\u001b[0m\n",
      "\u001b[35mSteps:  94%|█████████▍| 94/100 [35:28<02:17, 22.92s/it, lr=1e-6, step_loss=0.0236]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 94/100 [35:27<02:17, 22.92s/it, lr=1e-6, step_loss=0.0165]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 94/100 [35:37<02:17, 22.92s/it, lr=1e-6, step_loss=0.0247]\u001b[0m\n",
      "\u001b[35mSteps:  94%|█████████▍| 94/100 [35:37<02:17, 22.92s/it, lr=1e-6, step_loss=0.0573]\u001b[0m\n",
      "\u001b[35mSteps:  94%|█████████▍| 94/100 [35:42<02:17, 22.92s/it, lr=1e-6, step_loss=0.056]\u001b[0m\n",
      "\u001b[34mSteps:  94%|█████████▍| 94/100 [35:41<02:17, 22.92s/it, lr=1e-6, step_loss=0.0504]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 95/100 [35:46<01:54, 22.94s/it, lr=1e-6, step_loss=0.0504]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 95/100 [35:46<01:54, 22.94s/it, lr=1e-6, step_loss=0.0326]\u001b[0m\n",
      "\u001b[35mSteps:  95%|█████████▌| 95/100 [35:46<01:54, 22.94s/it, lr=1e-6, step_loss=0.056]\u001b[0m\n",
      "\u001b[35mSteps:  95%|█████████▌| 95/100 [35:46<01:54, 22.94s/it, lr=1e-6, step_loss=0.021]\u001b[0m\n",
      "\u001b[35mSteps:  95%|█████████▌| 95/100 [35:51<01:54, 22.94s/it, lr=1e-6, step_loss=0.11]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 95/100 [35:50<01:54, 22.94s/it, lr=1e-6, step_loss=0.102]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 95/100 [36:00<01:54, 22.94s/it, lr=1e-6, step_loss=0.00614]\u001b[0m\n",
      "\u001b[35mSteps:  95%|█████████▌| 95/100 [36:00<01:54, 22.94s/it, lr=1e-6, step_loss=0.124]\u001b[0m\n",
      "\u001b[35mSteps:  95%|█████████▌| 95/100 [36:05<01:54, 22.94s/it, lr=1e-6, step_loss=0.00854]\u001b[0m\n",
      "\u001b[34mSteps:  95%|█████████▌| 95/100 [36:04<01:54, 22.94s/it, lr=1e-6, step_loss=0.038]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 96/100 [36:09<01:31, 22.99s/it, lr=1e-6, step_loss=0.038]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 96/100 [36:09<01:31, 22.99s/it, lr=1e-6, step_loss=0.0214]\u001b[0m\n",
      "\u001b[35mSteps:  96%|█████████▌| 96/100 [36:09<01:31, 22.99s/it, lr=1e-6, step_loss=0.00854]\u001b[0m\n",
      "\u001b[35mSteps:  96%|█████████▌| 96/100 [36:09<01:31, 22.99s/it, lr=1e-6, step_loss=0.0636]\u001b[0m\n",
      "\u001b[35mSteps:  96%|█████████▌| 96/100 [36:14<01:31, 22.99s/it, lr=1e-6, step_loss=0.0257]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 96/100 [36:13<01:31, 22.99s/it, lr=1e-6, step_loss=0.0639]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 96/100 [36:23<01:31, 22.99s/it, lr=1e-6, step_loss=0.037]\u001b[0m\n",
      "\u001b[35mSteps:  96%|█████████▌| 96/100 [36:23<01:31, 22.99s/it, lr=1e-6, step_loss=0.0937]\u001b[0m\n",
      "\u001b[35mSteps:  96%|█████████▌| 96/100 [36:28<01:31, 22.99s/it, lr=1e-6, step_loss=0.00997]\u001b[0m\n",
      "\u001b[34mSteps:  96%|█████████▌| 96/100 [36:27<01:31, 22.99s/it, lr=1e-6, step_loss=0.0588]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 97/100 [36:32<01:08, 22.99s/it, lr=1e-6, step_loss=0.0588]\u001b[0m\n",
      "\u001b[34mSteps:  97%|█████████▋| 97/100 [36:32<01:08, 22.99s/it, lr=1e-6, step_loss=0.059]\u001b[0m\n",
      "\u001b[35mSteps:  97%|█████████▋| 97/100 [36:32<01:08, 22.99s/it, lr=1e-6, step_loss=0.00997]\u001b[0m\n",
      "\u001b[35mSteps:  97%|█████████▋| 97/100 [36:32<01:08, 22.99s/it, lr=1e-6, step_loss=0.0651]\u001b[0m\n",
      "\u001b[35mSteps:  98%|█████████▊| 98/100 [36:37<00:35, 17.64s/it, lr=1e-6, step_loss=0.0651]\u001b[0m\n",
      "\u001b[35mSteps:  98%|█████████▊| 98/100 [36:37<00:35, 17.64s/it, lr=1e-6, step_loss=0.0622]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 98/100 [36:37<00:35, 17.64s/it, lr=1e-6, step_loss=0.059]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 98/100 [36:37<00:35, 17.64s/it, lr=1e-6, step_loss=0.0542]\u001b[0m\n",
      "\u001b[35mSteps:  98%|█████████▊| 98/100 [36:51<00:35, 17.64s/it, lr=1e-6, step_loss=0.0396]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 98/100 [36:50<00:35, 17.64s/it, lr=1e-6, step_loss=0.068]\u001b[0m\n",
      "\u001b[35mSteps:  98%|█████████▊| 98/100 [36:56<00:35, 17.64s/it, lr=1e-6, step_loss=0.0675]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 98/100 [36:55<00:35, 17.64s/it, lr=1e-6, step_loss=0.0836]\u001b[0m\n",
      "\u001b[35mSteps:  98%|█████████▊| 98/100 [37:01<00:35, 17.64s/it, lr=1e-6, step_loss=0.0504]\u001b[0m\n",
      "\u001b[34mSteps:  98%|█████████▊| 98/100 [37:00<00:35, 17.64s/it, lr=1e-6, step_loss=0.0318]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 99/100 [37:05<00:20, 20.64s/it, lr=1e-6, step_loss=0.0318]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 99/100 [37:05<00:20, 20.64s/it, lr=1e-6, step_loss=0.0239]\u001b[0m\n",
      "\u001b[35mSteps:  99%|█████████▉| 99/100 [37:05<00:20, 20.64s/it, lr=1e-6, step_loss=0.0504]\u001b[0m\n",
      "\u001b[35mSteps:  99%|█████████▉| 99/100 [37:05<00:20, 20.64s/it, lr=1e-6, step_loss=0.0237]\u001b[0m\n",
      "\u001b[35mSteps:  99%|█████████▉| 99/100 [37:15<00:20, 20.64s/it, lr=1e-6, step_loss=0.041]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 99/100 [37:14<00:20, 20.64s/it, lr=1e-6, step_loss=0.0467]\u001b[0m\n",
      "\u001b[35mSteps:  99%|█████████▉| 99/100 [37:19<00:20, 20.64s/it, lr=1e-6, step_loss=0.0377]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 99/100 [37:18<00:20, 20.64s/it, lr=1e-6, step_loss=0.0152]\u001b[0m\n",
      "\u001b[35mSteps:  99%|█████████▉| 99/100 [37:24<00:20, 20.64s/it, lr=1e-6, step_loss=0.0617]\u001b[0m\n",
      "\u001b[34mSteps:  99%|█████████▉| 99/100 [37:23<00:20, 20.64s/it, lr=1e-6, step_loss=0.0796]\u001b[0m\n",
      "\u001b[35mSteps: 100%|██████████| 100/100 [37:28<00:00, 21.34s/it, lr=1e-6, step_loss=0.0617]\u001b[0m\n",
      "\u001b[34mSteps: 100%|██████████| 100/100 [37:28<00:00, 21.34s/it, lr=1e-6, step_loss=0.0796]\u001b[0m\n",
      "\u001b[34m03/22/2024 17:09:05 - INFO - accelerate.accelerator - Saving current state to /opt/ml/model/checkpoint-100\u001b[0m\n",
      "\u001b[34m03/22/2024 17:09:05 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:05,499] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:05,532] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /opt/ml/model/checkpoint-100/pytorch_model/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:05,532] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:11,578] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_1_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_3_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_2_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_4_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_6_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_5_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:11,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_7_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_7_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,535] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_7_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,535] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_6_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,541] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_6_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,552] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_4_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,553] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_4_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_5_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,553] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_5_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[35m[2024-03-22 17:09:15,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[35mSteps: 100%|██████████| 100/100 [37:39<00:00, 21.34s/it, lr=1e-6, step_loss=0.00854]\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_1_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,851] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_1_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,851] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,882] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,882] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,944] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_3_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,945] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_3_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:15,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:16,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_2_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:16,013] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /opt/ml/model/checkpoint-100/pytorch_model/zero_pp_rank_2_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2024-03-22 17:09:16,013] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!\u001b[0m\n",
      "\u001b[34m03/22/2024 17:09:16 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir /opt/ml/model/checkpoint-100/pytorch_model\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/checkpoint-100/unet/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/checkpoint-100/unet/diffusion_pytorch_model.safetensors\u001b[0m\n",
      "\u001b[34m03/22/2024 17:09:21 - INFO - accelerate.checkpointing - Scheduler state saved in /opt/ml/model/checkpoint-100/scheduler.bin\u001b[0m\n",
      "\u001b[34m03/22/2024 17:09:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /opt/ml/model/checkpoint-100/sampler.bin\u001b[0m\n",
      "\u001b[34m03/22/2024 17:09:21 - INFO - accelerate.checkpointing - Random states saved in /opt/ml/model/checkpoint-100/random_states_0.pkl\u001b[0m\n",
      "\u001b[34m03/22/2024 17:09:21 - INFO - __main__ - Saved state to /opt/ml/model/checkpoint-100\u001b[0m\n",
      "\u001b[34mSteps: 100%|██████████| 100/100 [37:44<00:00, 21.34s/it, lr=1e-6, step_loss=0.0702]\u001b[0m\n",
      "\u001b[35mSteps: 100%|██████████| 100/100 [37:45<00:00, 22.65s/it, lr=1e-6, step_loss=0.00854]\u001b[0m\n",
      "\u001b[34mLoading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  5.15it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoading pipeline components...:  71%|███████▏  | 5/7 [00:00<00:00, 10.73it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\u001b[0m\n",
      "\u001b[34mLoading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.90it/s]#033[A\u001b[0m\n",
      "\u001b[34mLoading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.44it/s]\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/vae/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/vae/diffusion_pytorch_model.safetensors\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/unet/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /opt/ml/model/unet/diffusion_pytorch_model.safetensors\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/scheduler/scheduler_config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /opt/ml/model/model_index.json\u001b[0m\n",
      "\u001b[34m0%|          | 0/25 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 1/25 [00:00<00:08,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 2/25 [00:00<00:08,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 3/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 4/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 5/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 6/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 7/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 8/25 [00:02<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 9/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 10/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 11/25 [00:03<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 12/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 13/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 14/25 [00:04<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 15/25 [00:05<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 16/25 [00:05<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 17/25 [00:05<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 18/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 19/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 20/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 21/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 22/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 23/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 24/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/25 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 1/25 [00:00<00:08,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 2/25 [00:00<00:08,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 3/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 4/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 5/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 6/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 7/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 8/25 [00:02<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 9/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 10/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 11/25 [00:03<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 12/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 13/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 14/25 [00:04<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 15/25 [00:05<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 16/25 [00:05<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 17/25 [00:05<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 18/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 19/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 20/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 21/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 22/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 23/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 24/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/25 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 1/25 [00:00<00:08,  2.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 2/25 [00:00<00:08,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 3/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 4/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 5/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 6/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 7/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 8/25 [00:02<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 9/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 10/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 11/25 [00:03<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 12/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 13/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 14/25 [00:04<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 15/25 [00:05<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 16/25 [00:05<00:03,  2.84it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 17/25 [00:05<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 18/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 19/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 20/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 21/25 [00:07<00:01,  2.84it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 22/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 23/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 24/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/25 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 1/25 [00:00<00:08,  2.83it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 2/25 [00:00<00:08,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 3/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 4/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 5/25 [00:01<00:07,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 6/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 7/25 [00:02<00:06,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 8/25 [00:02<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 9/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 10/25 [00:03<00:05,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 11/25 [00:03<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 12/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 13/25 [00:04<00:04,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 14/25 [00:04<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 15/25 [00:05<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 16/25 [00:05<00:03,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 17/25 [00:05<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 18/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 19/25 [00:06<00:02,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 20/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 21/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 22/25 [00:07<00:01,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 23/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 24/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 25/25 [00:08<00:00,  2.84it/s]\u001b[0m\n",
      "\u001b[34mSteps: 100%|██████████| 100/100 [38:32<00:00, 23.13s/it, lr=1e-6, step_loss=0.0702]\u001b[0m\n",
      "\u001b[35m2024-03-22 17:10:15,986 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-03-22 17:10:15,987 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-03-22 17:10:15,987 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m2024-03-22 17:10:15,988 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-22 17:10:15,988 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-22 17:10:15,989 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-03-22 17:10:24 Uploading - Uploading generated training model"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:1.13.1-gpu-py39-cu117-ubuntu20.04-sagemaker'\n",
    "\n",
    "instance_count = 2\n",
    "instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    'OUTPUT_DIR': '/opt/ml/model',\n",
    "    'MODEL_NAME': \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    'VAE_NAME': \"madebyollin/sdxl-vae-fp16-fix\",\n",
    "    'TRAIN_DIR': '/opt/ml/input/data/train'\n",
    "}\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='entry.py',\n",
    "                      source_dir='./sm_scripts',\n",
    "                      base_job_name='t2i-acc-launch-2',\n",
    "                      instance_count=instance_count,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      max_run=2*24*3600, #任务最大存续时间，默认2day，需要提交ticket提升quota最大28天\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "\n",
    "estimator.fit({'train': inputs_train})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991e31f-ba8a-4fb3-86d7-898deb7b5fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
